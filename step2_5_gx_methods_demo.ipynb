{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c74415b-a50d-4757-84ff-792c1f8d5e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# GX Different methods demo\n",
    "\n",
    "\n",
    "\n",
    "**steps**:\n",
    "- step1 shows installation\n",
    "- step2 setting context\n",
    "- step3 calls step2 to set context\n",
    "- step3 sql data source methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c9432de-e623-41f2-bdd2-c30ff3836bde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: great_expectations in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (1.5.7)\nRequirement already satisfied: sqlalchemy in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (2.0.42)\nRequirement already satisfied: pyodbc in /databricks/python3/lib/python3.11/site-packages (4.0.39)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.11/site-packages (1.5.3)\nRequirement already satisfied: altair<5.0.0,>=4.2.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (4.2.2)\nRequirement already satisfied: cryptography>=3.2 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (41.0.3)\nRequirement already satisfied: jinja2>=3 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (3.1.6)\nRequirement already satisfied: jsonschema>=2.5.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (4.25.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.7.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (3.26.1)\nRequirement already satisfied: mistune>=0.8.4 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (3.1.3)\nRequirement already satisfied: numpy>=1.22.4 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (1.23.5)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (23.2)\nRequirement already satisfied: posthog>3 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (6.3.2)\nRequirement already satisfied: pydantic>=1.10.7 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (2.11.7)\nRequirement already satisfied: pyparsing>=2.4 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (2.8.2)\nRequirement already satisfied: requests>=2.20 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (2.31.0)\nRequirement already satisfied: ruamel.yaml>=0.16 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (0.18.14)\nRequirement already satisfied: scipy>=1.6.0 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (1.11.1)\nRequirement already satisfied: tqdm>=4.59.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.1.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (4.14.1)\nRequirement already satisfied: tzlocal>=1.2 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (5.3.1)\nRequirement already satisfied: greenlet>=1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from sqlalchemy) (3.2.3)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas) (2022.7)\nRequirement already satisfied: entrypoints in /databricks/python3/lib/python3.11/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (0.4)\nRequirement already satisfied: toolz in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (1.0.0)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=3.2->great_expectations) (1.15.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jinja2>=3->great_expectations) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (0.26.0)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from posthog>3->great_expectations) (1.16.0)\nRequirement already satisfied: backoff>=1.10.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from posthog>3->great_expectations) (2.2.1)\nRequirement already satisfied: distro>=1.5.0 in /usr/lib/python3/dist-packages (from posthog>3->great_expectations) (1.7.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic>=1.10.7->great_expectations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic>=1.10.7->great_expectations) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic>=1.10.7->great_expectations) (0.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (2023.7.22)\nRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from ruamel.yaml>=0.16->great_expectations) (0.2.12)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.2->great_expectations) (2.21)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Run this in a separate cell before restarting\n",
    "%pip install great_expectations sqlalchemy pyodbc pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d960737-b7e5-4afc-ab8c-62b3d9e7b057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great Expectations 1.5.7 imported successfully\nDBFS context path: /dbfs/FileStore/great_expectations\nContext type: FileDataContext\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    import great_expectations as gx\n",
    "    print(f\"Great Expectations {gx.__version__} imported successfully\")\n",
    "    \n",
    "\n",
    "    is_databricks = False\n",
    "    try:\n",
    "        dbutils.fs.ls('/')\n",
    "        is_databricks = True\n",
    "\n",
    "    except NameError:\n",
    "\n",
    "        is_databricks = False\n",
    "    \n",
    "\n",
    "    if is_databricks:\n",
    "        dbfs_gx_path = \"/dbfs/FileStore/great_expectations\"\n",
    "        print(f\"DBFS context path: {dbfs_gx_path}\")\n",
    "        \n",
    "    \n",
    "        if os.path.exists(dbfs_gx_path):\n",
    "            context = gx.get_context(project_root_dir=dbfs_gx_path)\n",
    "           \n",
    "            print(f\"Context type: {type(context).__name__}\")\n",
    "        else:\n",
    "            print(\"DBFS context not found - creating new one\")\n",
    "            os.makedirs(dbfs_gx_path, exist_ok=True)\n",
    "            context = gx.get_context(project_root_dir=dbfs_gx_path)\n",
    "            print(\"New DBFS context created\")\n",
    "    else:\n",
    " \n",
    "        local_gx_path = os.path.join(os.getcwd(), \"great_expectations\")\n",
    "        context = gx.get_context(project_root_dir=local_gx_path)\n",
    "        print(\"Local context connected\")\n",
    "    \n",
    "\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Great Expectations import failed: {e}\")\n",
    "\n",
    "    raise ImportError(\"Great Expectations not available - run installation cell above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee91cbdf-0d46-4f61-9059-060969add2f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 context available: FileDataContext\nTotal methods discovered: 82\nGX context ready for pure GX method demonstrations\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import great_expectations as gx\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "demo_results = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"step\": \"step2_5_gx_methods_demo\",\n",
    "    \"gx_version\": gx.__version__,\n",
    "    \"methods_demonstrated\": [],\n",
    "    \"examples_completed\": 0,\n",
    "    \"status\": \"in_progress\",\n",
    "    \"focus\": \"pure_gx_methods_only\"\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    # Import json module explicitly for dbutils compatibility\n",
    "    import json\n",
    "    \n",
    "    step2_results = dbutils.notebook.run(\"./step2_gx_context_setup\", 0)\n",
    "    step2_data = json.loads(step2_results)\n",
    "    \n",
    "    if step2_data.get(\"context_created\"):\n",
    "        print(f\"Step 2 context available: {step2_data.get('context_type')}\")\n",
    "        discovered_methods = step2_data.get(\"context_methods\", [])\n",
    "        method_categories = step2_data.get(\"method_categories\", {})\n",
    "        print(f\"Total methods discovered: {len(discovered_methods)}\")\n",
    "    else:\n",
    "        print(f\"Step 2 context not available, creating fresh context\")\n",
    "        discovered_methods = []\n",
    "        method_categories = {}\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not load Step 2 results: {e}\")\n",
    "    discovered_methods = []\n",
    "    method_categories = {}\n",
    "\n",
    "\n",
    "try:\n",
    "    context = gx.get_context(project_root_dir=\"/dbfs/FileStore/great_expectations\")\n",
    "    print(f\"GX context ready for pure GX method demonstrations\")\n",
    "except Exception as e:\n",
    "    try:\n",
    "        context = gx.get_context(mode=\"ephemeral\")\n",
    "   \n",
    "    except Exception as e2:\n",
    "        print(f\"Could not create GX context: {e2}\")\n",
    "        context = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6090d850-5018-4ea3-bd09-8352ee0cbe2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # SQL DATASOURCE METHODS\n",
    "# # =============================================================================\n",
    "\n",
    "# if context is not None:\n",
    "\n",
    "    \n",
    "#     datasource_methods = method_categories.get(\"datasource\", [])\n",
    "\n",
    "    \n",
    "#     try:\n",
    "   \n",
    "  \n",
    "#         if hasattr(context, 'list_datasources'):\n",
    "#             datasources = context.list_datasources()\n",
    "#             print(f\"context.list_datasources() - Found {len(datasources)} datasources\")\n",
    "#             for ds in datasources[:3]: \n",
    "#                 ds_name = ds.get('name', 'unnamed') if isinstance(ds, dict) else str(ds)\n",
    "#                 print(f\"      • {ds_name}\")\n",
    "#             demo_results[\"methods_demonstrated\"].append(\"list_datasources\")\n",
    "#         else:\n",
    "#             print(f\"list_datasources method not available\")\n",
    "        \n",
    "    \n",
    "#         print(f\"GX SQL DATA SOURCES\")\n",
    "#         if hasattr(context, 'data_sources'):\n",
    "#             print(f\"context.data_sources available\")\n",
    "            \n",
    "      \n",
    "#             try:\n",
    "       \n",
    "#                 data_sources_list = list(context.data_sources)\n",
    "#                 print(f\"context.data_sources (iteration) - Found {len(data_sources_list)} sources\")\n",
    "#                 demo_results[\"methods_demonstrated\"].append(\"data_sources_iteration\")\n",
    "#             except Exception as e1:\n",
    "#                 try:\n",
    "\n",
    "#                     all_datasources = context.list_datasources()\n",
    "#                     print(f\"context.list_datasources() - Found {len(all_datasources)} sources (legacy method)\")\n",
    "#                     demo_results[\"methods_demonstrated\"].append(\"list_datasources_fallback\")\n",
    "#                 except Exception as e2:\n",
    "#                     print(f\"data_sources listing failed: {e1}\")\n",
    "#                     print(f\"legacy fallback failed: {e2}\")\n",
    "            \n",
    "\n",
    "#             if hasattr(context.data_sources, 'add_sql'):\n",
    "#                 print(f\"Available data_sources methods detected:\")\n",
    "#                 methods_list = [\n",
    "#                     'add_sql', 'add_postgres', 'add_sqlite', 'add_databricks_sql', \n",
    "#                     'add_snowflake', 'add_bigquery', 'add_redshift'\n",
    "#                 ]\n",
    "#                 for method in methods_list:\n",
    "#                     if hasattr(context.data_sources, method):\n",
    "#                         print(f\"      • {method}() - SQL database support\")\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "#             if hasattr(context.data_sources, 'add_sql'):\n",
    "#                 print(f\"data_sources.add_sql() - Available\")\n",
    "#                 demo_results[\"methods_demonstrated\"].append(\"data_sources.add_sql\")\n",
    "#             else:\n",
    "#                 print(f\"data_sources.add_sql() - Not available\")\n",
    "        \n",
    "\n",
    "  \n",
    "#         legacy_methods = ['add_datasource', 'get_datasource', 'delete_datasource']\n",
    "        \n",
    "#         for method_name in legacy_methods:\n",
    "#             if hasattr(context, method_name):\n",
    "#                 print(f\"context.{method_name}() - Available\")\n",
    "#                 demo_results[\"methods_demonstrated\"].append(method_name)\n",
    "#             else:\n",
    "#                 print(f\"context.{method_name}() - Not available\")\n",
    "        \n",
    "       \n",
    "        \n",
    "#         demo_results[\"examples_completed\"] += 1\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Datasource methods demonstration failed: {e}\")\n",
    "\n",
    "# else:\n",
    "#     print(f\"No context available for datasource demonstrations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b07bdb7c-d756-416b-b9d0-bc339017a5de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEXPECTATION SUITE METHODS DEMONSTRATION\n--------------------------------------------------\nFound 3 expectation methods\n\n1. MODERN EXPECTATION SUITES (GX 1.5.5+)\n   context.suites available\n   Legacy expectation suites - Found 0 suites\n   Creating demo expectation suite...\n   suite creation failed: Cannot add ExpectationSuite with name demo_suite because it already exists.\n   ExpectationConfiguration import successful\n   Example expectation configurations:\n      • Table row count: expect_table_row_count_to_be_between\n        Parameters: {'min_value': 1, 'max_value': 10000}\n      • Column existence: expect_column_to_exist\n        Parameters: {'column': 'id'}\n      • Non-null values: expect_column_values_to_not_be_null\n        Parameters: {'column': 'name'}\n      • Unique values: expect_column_values_to_be_unique\n        Parameters: {'column': 'email'}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPECTATION SUITE METHODS DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "if context is not None:\n",
    "    print(\"\\nEXPECTATION SUITE METHODS DEMONSTRATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    expectation_methods = method_categories.get(\"expectation\", [])\n",
    "    print(f\"Found {len(expectation_methods)} expectation methods\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Modern GX 1.5.5+ suites approach\n",
    "        print(\"\\n1. MODERN EXPECTATION SUITES (GX 1.5.5+)\")\n",
    "        if hasattr(context, 'suites'):\n",
    "            print(\"   context.suites available\")\n",
    "            \n",
    "            try:\n",
    "                suites_list = list(context.suites)\n",
    "                print(f\"   context.suites (iteration) - Found {len(suites_list)} suites\")\n",
    "                for suite in suites_list[:3]:\n",
    "                    suite_name = suite.name if hasattr(suite, 'name') else str(suite)\n",
    "                    print(f\"      - {suite_name}\")\n",
    "                demo_results[\"methods_demonstrated\"].append(\"suites_iteration\")\n",
    "            except Exception as e1:\n",
    "                try:\n",
    "                    legacy_suites = context.list_expectation_suites() if hasattr(context, 'list_expectation_suites') else []\n",
    "                    print(f\"   Legacy expectation suites - Found {len(legacy_suites)} suites\")\n",
    "                    demo_results[\"methods_demonstrated\"].append(\"legacy_suites_list\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"   suites listing failed: {e1}\")\n",
    "                    print(f\"   legacy fallback failed: {e2}\")\n",
    "                    print(\"   Suites exist but listing method differs in GX 1.5.5+\")\n",
    "            \n",
    "            try:\n",
    "                print(\"   Creating demo expectation suite...\")\n",
    "                from great_expectations.core import ExpectationSuite\n",
    "                demo_suite = context.suites.add(ExpectationSuite(name=\"demo_suite\"))\n",
    "                print(\"   suites.add() - Created demo suite\")\n",
    "                \n",
    "                if hasattr(demo_suite, 'add_expectation'):\n",
    "                    print(\"   Suite has add_expectation capability\")\n",
    "                \n",
    "                demo_results[\"methods_demonstrated\"].append(\"suites.add\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   suite creation failed: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        expectation_config_available = False\n",
    "        try:\n",
    "            from great_expectations.expectations.expectation_configuration import ExpectationConfiguration\n",
    "            expectation_config_available = True\n",
    "            print(\"   ExpectationConfiguration import successful\")\n",
    "        except ImportError:\n",
    "            try:\n",
    "                from great_expectations.core.expectation_configuration import ExpectationConfiguration\n",
    "                expectation_config_available = True\n",
    "                print(f\"   ExpectationConfiguration import (legacy path) successful\")\n",
    "            except ImportError:\n",
    "                print(f\"   ExpectationConfiguration import failed\")\n",
    "        \n",
    "        if expectation_config_available:\n",
    "            print(f\"   Example expectation configurations:\")\n",
    "            \n",
    "            example_expectations = [\n",
    "                {\n",
    "                    \"name\": \"Table row count\",\n",
    "                    \"type\": \"expect_table_row_count_to_be_between\",\n",
    "                    \"kwargs\": {\"min_value\": 1, \"max_value\": 10000}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Column existence\",\n",
    "                    \"type\": \"expect_column_to_exist\", \n",
    "                    \"kwargs\": {\"column\": \"id\"}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Non-null values\",\n",
    "                    \"type\": \"expect_column_values_to_not_be_null\",\n",
    "                    \"kwargs\": {\"column\": \"name\"}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Unique values\",\n",
    "                    \"type\": \"expect_column_values_to_be_unique\",\n",
    "                    \"kwargs\": {\"column\": \"email\"}\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            for exp in example_expectations:\n",
    "                print(f\"      • {exp['name']}: {exp['type']}\")\n",
    "                print(f\"        Parameters: {exp['kwargs']}\")\n",
    "        \n",
    "        demo_results[\"examples_completed\"] += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Expectation methods demonstration failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"No context available for expectation demonstrations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a45708-4004-436b-8b53-4c602aa35f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nVALIDATION METHODS DEMONSTRATION\n--------------------------------------------------\nFound 9 validation methods\n\n1. VALIDATOR CREATION METHODS\n   context.get_validator() - Available\n   context.build_batch_request() - Not available\n   context.get_batch_list() - Not available\n\n2. CORE EXPECTATION METHODS\n   Core GX Expectations Available:\n      - expect_table_row_count_to_be_between\n      - expect_table_row_count_to_equal\n      - expect_column_to_exist\n      - expect_column_values_to_not_be_null\n      - expect_column_values_to_be_unique\n      - expect_column_values_to_be_in_set\n      - expect_column_values_to_match_regex\n      - expect_column_mean_to_be_between\n      - expect_column_sum_to_be_between\n      - expect_column_min_to_be_between\n      - expect_column_max_to_be_between\n\n3. SQL VALIDATION PATTERNS\n   Table-level Validations:\n      - Row count validation: expect_table_row_count_to_be_between(min_value=1, max_value=10000)\n      - Table existence: Built into datasource asset creation\n      - Schema validation: expect_table_columns_to_match_ordered_list()\n   Column-level Validations:\n      - Null checks: expect_column_values_to_not_be_null(column='HIERARCHY_ID')\n      • Uniqueness: expect_column_values_to_be_unique(column='HIERARCHY_ID')\n      • Data types: expect_column_values_to_be_of_type(column='date_col', type_='datetime')\n      • Value ranges: expect_column_values_to_be_between(column='age', min_value=0, max_value=120)\nBusiness Rule Validations:\n      • Custom SQL: expect_column_pair_values_A_to_be_greater_than_B()\n      • Regex patterns: expect_column_values_to_match_regex(column='email', regex=r'^[^@]+@[^@]+\\.[^@]+$')\n      • Set membership: expect_column_values_to_be_in_set(column='status', value_set=['active', 'inactive'])\nBATCH REQUEST PATTERNS\nSQL Table Asset Batch Request:\n      • asset.build_batch_request()\n      • asset.build_batch_request(options={'year': '2024'})\n      • asset.build_batch_request(batch_slice={'start': 0, 'stop': 1000})\nQuery Asset Batch Request:\n      • query_asset.build_batch_request()\n      • Dynamic parameter binding for filtered queries\nVALIDATION RESULT HANDLING\nResult Object Properties:\n      • result.success - Boolean success/failure\n      • result.result - Detailed validation metrics\n      • result.exception_info - Error details if failed\n      • result.meta - Metadata about the validation\nCheckpoint Execution:\n      • checkpoint.run() - Execute full validation suite\n      • checkpoint_result.run_results - Individual validation results\n      • checkpoint_result.success - Overall success status\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VALIDATION METHODS DEMONSTRATION (PURE GX)\n",
    "# =============================================================================\n",
    "\n",
    "if context is not None:\n",
    "    print(\"\\nVALIDATION METHODS DEMONSTRATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    validation_methods = method_categories.get(\"validation\", [])\n",
    "    print(f\"Found {len(validation_methods)} validation methods\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Validator creation methods\n",
    "        print(\"\\n1. VALIDATOR CREATION METHODS\")\n",
    "        validator_methods = ['get_validator', 'build_batch_request', 'get_batch_list']\n",
    "        \n",
    "        for method_name in validator_methods:\n",
    "            if hasattr(context, method_name):\n",
    "                print(f\"   context.{method_name}() - Available\")\n",
    "                demo_results[\"methods_demonstrated\"].append(method_name)\n",
    "            else:\n",
    "                print(f\"   context.{method_name}() - Not available\")\n",
    "        \n",
    "        # 2. Pure GX Expectation Methods (without pandas data)\n",
    "        print(\"\\n2. CORE EXPECTATION METHODS\")\n",
    "        \n",
    "        core_expectations = [\n",
    "            \"expect_table_row_count_to_be_between\",\n",
    "            \"expect_table_row_count_to_equal\", \n",
    "            \"expect_column_to_exist\",\n",
    "            \"expect_column_values_to_not_be_null\",\n",
    "            \"expect_column_values_to_be_unique\",\n",
    "            \"expect_column_values_to_be_in_set\",\n",
    "            \"expect_column_values_to_match_regex\",\n",
    "            \"expect_column_mean_to_be_between\",\n",
    "            \"expect_column_sum_to_be_between\",\n",
    "            \"expect_column_min_to_be_between\",\n",
    "            \"expect_column_max_to_be_between\"\n",
    "        ]\n",
    "        \n",
    "        print(\"   Core GX Expectations Available:\")\n",
    "        for exp in core_expectations:\n",
    "            print(f\"      - {exp}\")\n",
    "        \n",
    "        demo_results[\"methods_demonstrated\"].extend(core_expectations)\n",
    "        \n",
    "        # 3. SQL-specific validation patterns\n",
    "        print(\"\\n3. SQL VALIDATION PATTERNS\")\n",
    "        \n",
    "        print(\"   Table-level Validations:\")\n",
    "        print(\"      - Row count validation: expect_table_row_count_to_be_between(min_value=1, max_value=10000)\")\n",
    "        print(\"      - Table existence: Built into datasource asset creation\")\n",
    "        print(\"      - Schema validation: expect_table_columns_to_match_ordered_list()\")\n",
    "        \n",
    "        print(\"   Column-level Validations:\")\n",
    "        print(\"      - Null checks: expect_column_values_to_not_be_null(column='HIERARCHY_ID')\")\n",
    "        print(f\"      • Uniqueness: expect_column_values_to_be_unique(column='HIERARCHY_ID')\")\n",
    "        print(f\"      • Data types: expect_column_values_to_be_of_type(column='date_col', type_='datetime')\")\n",
    "        print(f\"      • Value ranges: expect_column_values_to_be_between(column='age', min_value=0, max_value=120)\")\n",
    "        \n",
    "        print(f\"Business Rule Validations:\")\n",
    "        print(f\"      • Custom SQL: expect_column_pair_values_A_to_be_greater_than_B()\")\n",
    "        print(f\"      • Regex patterns: expect_column_values_to_match_regex(column='email', regex=r'^[^@]+@[^@]+\\\\.[^@]+$')\")\n",
    "        print(f\"      • Set membership: expect_column_values_to_be_in_set(column='status', value_set=['active', 'inactive'])\")\n",
    "        \n",
    "        # 4. Batch request patterns (without actual data)\n",
    "        print(f\"BATCH REQUEST PATTERNS\")\n",
    "        \n",
    "        print(f\"SQL Table Asset Batch Request:\")\n",
    "        print(f\"      • asset.build_batch_request()\")\n",
    "        print(f\"      • asset.build_batch_request(options={{'year': '2024'}})\")\n",
    "        print(f\"      • asset.build_batch_request(batch_slice={{'start': 0, 'stop': 1000}})\")\n",
    "        \n",
    "        print(f\"Query Asset Batch Request:\")\n",
    "        print(f\"      • query_asset.build_batch_request()\")\n",
    "        print(f\"      • Dynamic parameter binding for filtered queries\")\n",
    "        \n",
    "        # 5. Validation result handling\n",
    "        print(f\"VALIDATION RESULT HANDLING\")\n",
    "        \n",
    "        print(f\"Result Object Properties:\")\n",
    "        print(f\"      • result.success - Boolean success/failure\")\n",
    "        print(f\"      • result.result - Detailed validation metrics\")\n",
    "        print(f\"      • result.exception_info - Error details if failed\")\n",
    "        print(f\"      • result.meta - Metadata about the validation\")\n",
    "        \n",
    "        print(f\"Checkpoint Execution:\")\n",
    "        print(f\"      • checkpoint.run() - Execute full validation suite\")\n",
    "        print(f\"      • checkpoint_result.run_results - Individual validation results\")\n",
    "        print(f\"      • checkpoint_result.success - Overall success status\")\n",
    "        \n",
    "        demo_results[\"examples_completed\"] += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Validation methods demonstration failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"No context available for validation demonstrations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf4c9f1-975a-4b67-ab25-60b39a02cd3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA DOCS METHODS\ncontext.build_data_docs() - Available\ncontext.get_docs_sites_urls() - Available\nAttempting to build data docs...\nData docs built successfully\nCHECKPOINT METHODS\ncontext.checkpoints available\nLegacy checkpoints - Found 0 checkpoints\ncontext.list_checkpoints() - Not available\ncontext.get_checkpoint() - Not available\ncontext.add_checkpoint() - Not available\nSTORE METHODS\nFound 7 store methods:\n      - add_store\n      - checkpoint_store\n      - checkpoint_store_name\n      - delete_store\n      - list_active_stores\n      - list_stores\n      - stores\ncontext.expectations_store - Available\ncontext.validations_store - Not available\ncontext.checkpoint_store - Available\n"
     ]
    }
   ],
   "source": [
    "if context is not None:\n",
    "    try:\n",
    "        # 1. Data Docs methods\n",
    "        print(\"DATA DOCS METHODS\")\n",
    "        data_docs_methods = ['build_data_docs', 'get_docs_sites_urls']\n",
    "        \n",
    "        for method_name in data_docs_methods:\n",
    "            if hasattr(context, method_name):\n",
    "                print(f\"context.{method_name}() - Available\")\n",
    "                demo_results[\"methods_demonstrated\"].append(method_name)\n",
    "            else:\n",
    "                print(f\"context.{method_name}() - Not available\")\n",
    "        \n",
    "        if hasattr(context, 'build_data_docs'):\n",
    "            try:\n",
    "                print(\"Attempting to build data docs...\")\n",
    "                context.build_data_docs()\n",
    "                print(\"Data docs built successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Data docs build failed: {e}\")\n",
    "        \n",
    "        # 2. Checkpoint methods\n",
    "        print(\"CHECKPOINT METHODS\")\n",
    "        \n",
    "        if hasattr(context, 'checkpoints'):\n",
    "            print(\"context.checkpoints available\")\n",
    "            try:\n",
    "                checkpoints_list = list(context.checkpoints)\n",
    "                print(f\"context.checkpoints (iteration) - Found {len(checkpoints_list)} checkpoints\")\n",
    "                demo_results[\"methods_demonstrated\"].append(\"checkpoints_iteration\")\n",
    "            except Exception as e1:\n",
    "                try:\n",
    "                    legacy_checkpoints = context.list_checkpoints() if hasattr(context, 'list_checkpoints') else []\n",
    "                    print(f\"Legacy checkpoints - Found {len(legacy_checkpoints)} checkpoints\")\n",
    "                    demo_results[\"methods_demonstrated\"].append(\"legacy_checkpoints_list\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"checkpoints listing failed: {e1}\")\n",
    "                    print(f\"legacy fallback failed: {e2}\")\n",
    "                    print(\"Checkpoints exist but listing method differs in GX 1.5.5+\")\n",
    "        \n",
    "        checkpoint_methods = ['list_checkpoints', 'get_checkpoint', 'add_checkpoint']\n",
    "        for method_name in checkpoint_methods:\n",
    "            if hasattr(context, method_name):\n",
    "                print(f\"context.{method_name}() - Available\")\n",
    "                demo_results[\"methods_demonstrated\"].append(method_name)\n",
    "            else:\n",
    "                print(f\"context.{method_name}() - Not available\")\n",
    "        \n",
    "        # 3. Store methods\n",
    "        print(\"STORE METHODS\")\n",
    "        store_methods = method_categories.get(\"store\", [])\n",
    "        \n",
    "        if store_methods:\n",
    "            print(f\"Found {len(store_methods)} store methods:\")\n",
    "            for method in store_methods[:100]:\n",
    "                print(f\"      - {method}\")\n",
    "        else:\n",
    "            print(\"\")\n",
    "        \n",
    "        store_attributes = ['expectations_store', 'validations_store', 'checkpoint_store']\n",
    "        for attr in store_attributes:\n",
    "            if hasattr(context, attr):\n",
    "                print(f\"context.{attr} - Available\")\n",
    "            else:\n",
    "                print(f\"context.{attr} - Not available\")\n",
    "        \n",
    "        demo_results[\"examples_completed\"] += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Data docs & checkpoint demonstration failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"No context available for data docs demonstrations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f7b4b9-8978-44da-9cfb-25b312914930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Final statistics\n",
    "total_methods_demonstrated = len(set(demo_results[\"methods_demonstrated\"]))\n",
    "examples_completed = demo_results[\"examples_completed\"]\n",
    "\n",
    "print(f\"GX methods demonstrated: {total_methods_demonstrated}\")\n",
    "print(f\"Example sections completed: {examples_completed}\")\n",
    "\n",
    "\n",
    "# Categorize demonstrated methods\n",
    "demonstrated_by_category = {}\n",
    "for method in demo_results[\"methods_demonstrated\"]:\n",
    "    categorized = False\n",
    "    for category, methods in method_categories.items():\n",
    "        if method in methods:\n",
    "            if category not in demonstrated_by_category:\n",
    "                demonstrated_by_category[category] = []\n",
    "            demonstrated_by_category[category].append(method)\n",
    "            categorized = True\n",
    "            break\n",
    "    \n",
    "    if not categorized:\n",
    "        if \"core_expectations\" not in demonstrated_by_category:\n",
    "            demonstrated_by_category[\"core_expectations\"] = []\n",
    "        demonstrated_by_category[\"core_expectations\"].append(method)\n",
    "\n",
    "print(f\"GX METHODS BY CATEGORY:\")\n",
    "for category, methods in demonstrated_by_category.items():\n",
    "    print(f\"   {category.upper()}: {len(methods)} methods\")\n",
    "    for method in sorted(methods)[:5]:  # Show first 5\n",
    "        print(f\"      • {method}\")\n",
    "    if len(methods) > 5:\n",
    "        print(f\"      ... and {len(methods) - 5} more\")\n",
    "\n",
    "# Key learnings and recommendations for SQL-focused GX usage\n",
    "print(f\"KEY LEARNINGS & RECOMMENDATIONS (SQL FOCUS):\")\n",
    "print(f\"   1. GX {gx.__version__} excels at SQL database validation\")\n",
    "print(f\"   2. context.data_sources.add_sql() for database connections\")\n",
    "print(f\"   3. SQL table/query assets for flexible data access\")\n",
    "print(f\"   4. Rich expectation library for business rules\")\n",
    "print(f\"   5. NO pandas dependency required for SQL workflows\")\n",
    "\n",
    "# SQL-specific usage patterns for GX\n",
    "print(f\"SQL-FOCUSED USAGE PATTERNS FOR GX {gx.__version__}:\")\n",
    "print(f\"   • Data Sources: context.data_sources.add_sql(connection_string='...')\")\n",
    "print(f\"   • Table Assets: sql_datasource.add_table_asset(table_name='dbo.TableName')\")\n",
    "print(f\"   • Query Assets: sql_datasource.add_query_asset(query='SELECT ...')\")\n",
    "print(f\"   • Suites: context.suites.add() / get()\")\n",
    "print(f\"   • Validation: context.get_validator(batch_request=...)\")\n",
    "print(f\"   • Documentation: context.build_data_docs()\")\n",
    "\n",
    "# AMS Data Quality Dashboard specific recommendations\n",
    "print(f\"AMS DATA QUALITY DASHBOARD RECOMMENDATIONS:\")\n",
    "print(f\"   1. Use SQL Server datasource with ODBC connection\")\n",
    "print(f\"   2. Create table assets for dbo.DQ_LOGIC and related tables\")\n",
    "print(f\"   3. Build expectation suites for data quality rules\")\n",
    "print(f\"   4. Implement hierarchy validation with expect_column_values_to_be_unique\")\n",
    "print(f\"   5. Use expect_table_row_count_to_be_between for data volume checks\")\n",
    "print(f\"   6. Apply expect_column_values_to_not_be_null for critical fields\")\n",
    "\n",
    "# Production deployment patterns\n",
    "print(f\"\\n\uD83D\uDE80 PRODUCTION DEPLOYMENT PATTERNS:\")\n",
    "print(f\"   1. Store connection strings securely (Azure Key Vault)\")\n",
    "print(f\"   2. Use checkpoints for automated validation execution\")\n",
    "print(f\"   3. Generate data docs for stakeholder communication\")\n",
    "print(f\"   4. Integrate with CI/CD pipelines for continuous validation\")\n",
    "print(f\"   5. Set up alerting based on validation results\")\n",
    "print(f\"   6. Version control expectation suites with Git\")\n",
    "\n",
    "# Final status\n",
    "demo_results[\"status\"] = \"success\"\n",
    "demo_results[\"total_methods_demonstrated\"] = total_methods_demonstrated\n",
    "demo_results[\"demonstrated_by_category\"] = demonstrated_by_category\n",
    "demo_results[\"sql_focused\"] = True\n",
    "demo_results[\"pandas_dependency\"] = False\n",
    "\n",
    "print(f\"STEP 2.5 COMPLETED SUCCESSFULLY\")\n",
    "print(f\"Comprehensive PURE GX methods demonstration completed\")\n",
    "print(f\"{total_methods_demonstrated} SQL-focused methods explored\")\n",
    "print(f\"Zero pandas dependencies - Pure Great Expectations only\")\n",
    "print(f\"Ready for AMS Data Quality Dashboard implementation\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clean and return results\n",
    "def clean_for_json(obj):\n",
    "    \"\"\"Convert non-serializable types to JSON-compatible types\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_for_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_for_json(v) for v in obj]\n",
    "    elif hasattr(obj, 'isoformat'):  # datetime objects\n",
    "        return str(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "clean_results = clean_for_json(demo_results)\n",
    "dbutils.notebook.exit(json.dumps(clean_results))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "step2_5_gx_methods_demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}