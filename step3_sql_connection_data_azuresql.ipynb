{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2126faa6-47bc-42a5-b8e5-a05e20592a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 3: SQL Data Loading & Preparation\n",
    "\n",
    "**Purpose**: Load target data and prepare for Great Expectations validation\n",
    "\n",
    "**Key Activities**:\n",
    "- Load target table (DQ_LOGIC) data using existing connection\n",
    "- Analyze table schema and data structure\n",
    "- Profile data quality and completeness\n",
    "- Prepare DataFrame for GX validation\n",
    "\n",
    "**Expected Outputs**:\n",
    "- Loaded DataFrame with target data\n",
    "- Schema analysis and column information\n",
    "- Data profiling summary\n",
    "- Readiness status for step 4\n",
    "\n",
    "**Note**: Assumes SQL Server connection is already established and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09449865-e866-48f4-8251-aec653ca695f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting great_expectations\n  Using cached great_expectations-1.5.6-py3-none-any.whl.metadata (8.8 kB)\nCollecting sqlalchemy\n  Using cached sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nRequirement already satisfied: pyodbc in /databricks/python3/lib/python3.12/site-packages (5.0.1)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (1.5.3)\nCollecting altair<5.0.0,>=4.2.1 (from great_expectations)\n  Using cached altair-4.2.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: cryptography>=3.2 in /databricks/python3/lib/python3.12/site-packages (from great_expectations) (42.0.5)\nCollecting jinja2>=3 (from great_expectations)\n  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\nCollecting jsonschema>=2.5.1 (from great_expectations)\n  Using cached jsonschema-4.25.0-py3-none-any.whl.metadata (7.7 kB)\nCollecting marshmallow<4.0.0,>=3.7.1 (from great_expectations)\n  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting mistune>=0.8.4 (from great_expectations)\n  Using cached mistune-3.1.3-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from great_expectations) (24.1)\nCollecting posthog<6,>3 (from great_expectations)\n  Using cached posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: pydantic>=1.10.7 in /databricks/python3/lib/python3.12/site-packages (from great_expectations) (2.8.2)\nRequirement already satisfied: pyparsing>=2.4 in /databricks/python3/lib/python3.12/site-packages (from great_expectations) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.12/site-packages (from great_expectations) (2.9.0.post0)\nRequirement already satisfied: requests>=2.20 in /databricks/python3/lib/python3.12/site-packages (from great_expectations) (2.32.2)\nCollecting ruamel.yaml>=0.16 (from great_expectations)\n  Using cached ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: scipy>=1.6.0 in /databricks/python3/lib/python3.12/site-packages (from great_expectations) (1.13.1)\nCollecting tqdm>=4.59.0 (from great_expectations)\n  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nRequirement already satisfied: typing-extensions>=4.1.0 in /databricks/python3/lib/python3.12/site-packages (from great_expectations) (4.11.0)\nCollecting tzlocal>=1.2 (from great_expectations)\n  Using cached tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: numpy>=1.22.4 in /databricks/python3/lib/python3.12/site-packages (from great_expectations) (1.26.4)\nCollecting greenlet>=1 (from sqlalchemy)\n  Using cached greenlet-3.2.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2024.1)\nCollecting entrypoints (from altair<5.0.0,>=4.2.1->great_expectations)\n  Using cached entrypoints-0.4-py3-none-any.whl.metadata (2.6 kB)\nCollecting toolz (from altair<5.0.0,>=4.2.1->great_expectations)\n  Using cached toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography>=3.2->great_expectations) (1.16.0)\nCollecting MarkupSafe>=2.0 (from jinja2>=3->great_expectations)\n  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting attrs>=22.2.0 (from jsonschema>=2.5.1->great_expectations)\n  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\nCollecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.5.1->great_expectations)\n  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting referencing>=0.28.4 (from jsonschema>=2.5.1->great_expectations)\n  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\nCollecting rpds-py>=0.7.1 (from jsonschema>=2.5.1->great_expectations)\n  Using cached rpds_py-0.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from posthog<6,>3->great_expectations) (1.16.0)\nCollecting backoff>=1.10.0 (from posthog<6,>3->great_expectations)\n  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: distro>=1.5.0 in /usr/lib/python3/dist-packages (from posthog<6,>3->great_expectations) (1.9.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic>=1.10.7->great_expectations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /databricks/python3/lib/python3.12/site-packages (from pydantic>=1.10.7->great_expectations) (2.20.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.20->great_expectations) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.20->great_expectations) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.20->great_expectations) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.20->great_expectations) (2024.6.2)\nCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.16->great_expectations)\n  Using cached ruamel.yaml.clib-0.2.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=3.2->great_expectations) (2.21)\nUsing cached great_expectations-1.5.6-py3-none-any.whl (4.9 MB)\nUsing cached sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\nUsing cached altair-4.2.2-py3-none-any.whl (813 kB)\nUsing cached greenlet-3.2.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (605 kB)\nUsing cached jinja2-3.1.6-py3-none-any.whl (134 kB)\nUsing cached jsonschema-4.25.0-py3-none-any.whl (89 kB)\nUsing cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\nUsing cached mistune-3.1.3-py3-none-any.whl (53 kB)\nUsing cached posthog-5.4.0-py3-none-any.whl (105 kB)\nUsing cached ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\nUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\nUsing cached tzlocal-5.3.1-py3-none-any.whl (18 kB)\nUsing cached attrs-25.3.0-py3-none-any.whl (63 kB)\nUsing cached backoff-2.2.1-py3-none-any.whl (15 kB)\nUsing cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\nUsing cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nUsing cached referencing-0.36.2-py3-none-any.whl (26 kB)\nUsing cached rpds_py-0.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (386 kB)\nUsing cached ruamel.yaml.clib-0.2.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (754 kB)\nUsing cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\nUsing cached toolz-1.0.0-py3-none-any.whl (56 kB)\nInstalling collected packages: tzlocal, tqdm, toolz, ruamel.yaml.clib, rpds-py, mistune, marshmallow, MarkupSafe, greenlet, entrypoints, backoff, attrs, sqlalchemy, ruamel.yaml, referencing, posthog, jinja2, jsonschema-specifications, jsonschema, altair, great_expectations\nSuccessfully installed MarkupSafe-3.0.2 altair-4.2.2 attrs-25.3.0 backoff-2.2.1 entrypoints-0.4 great_expectations-1.5.6 greenlet-3.2.3 jinja2-3.1.6 jsonschema-4.25.0 jsonschema-specifications-2025.4.1 marshmallow-3.26.1 mistune-3.1.3 posthog-5.4.0 referencing-0.36.2 rpds-py-0.26.0 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 sqlalchemy-2.0.41 toolz-1.0.0 tqdm-4.67.1 tzlocal-5.3.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Run this in a separate cell before restarting\n",
    "%pip install great_expectations sqlalchemy pyodbc pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0972c6bc-a01a-4c44-89c6-98acc9ce8c03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBFS Root: /dbfs/FileStore/shared_uploads/great_expectations\nDBFS Display: /FileStore/shared_uploads/great_expectations\nCREATING GX STRUCTURE IN DBFS\n----------------------------------------\nCreated: expectations/\nCreated: checkpoints/\nCreated: data_docs/\nCreated: validations/\nCreated: profiling/\nCreated: uncommitted/\nDBFS verification successful: 6 items found\n\nINITIALIZING GX CONTEXT\n----------------------------------------\nContext type: FileDataContext\nPersistent storage: /dbfs/FileStore/shared_uploads/great_expectations\n\nCONTEXT VERIFICATION\n----------------------------------------\nAvailable methods: 3/3\n\nCONTEXT\nType: FileDataContext\n\nEXPORTING CONTEXT\n----------------------------------------\nContext exported as 'gx_context'\nDBFS path exported as 'DBFS_GX_ROOT'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# UNIVERSAL DBFS CONTEXT SETUP\n",
    "# Copy this cell to ALL notebooks that need GX context (Steps 2-8)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "import os\n",
    "import great_expectations as gx\n",
    "\n",
    "\n",
    "\n",
    "DBFS_GX_ROOT = \"/dbfs/FileStore/shared_uploads/great_expectations\"\n",
    "DBFS_DISPLAY_PATH = \"/FileStore/shared_uploads/great_expectations\"\n",
    "\n",
    "print(f\"DBFS Root: {DBFS_GX_ROOT}\")\n",
    "print(f\"DBFS Display: {DBFS_DISPLAY_PATH}\")\n",
    "\n",
    "# Step 2: Create DBFS directory structure\n",
    "print(\"CREATING GX STRUCTURE IN DBFS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    os.makedirs(DBFS_GX_ROOT, exist_ok=True)\n",
    "    \n",
    "    subdirs = [\n",
    "        \"expectations\",\n",
    "        \"checkpoints\", \n",
    "        \"data_docs\",\n",
    "        \"validations\",\n",
    "        \"profiling\",\n",
    "        \"uncommitted\"\n",
    "    ]\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(DBFS_GX_ROOT, subdir)\n",
    "        os.makedirs(subdir_path, exist_ok=True)\n",
    "        print(f\"Created: {subdir}/\")\n",
    "    \n",
    "    try:\n",
    "        files = dbutils.fs.ls(DBFS_DISPLAY_PATH)\n",
    "        print(f\"DBFS verification successful: {len(files)} items found\")\n",
    "    except Exception as dbfs_error:\n",
    "        print(f\"DBFS verification warning: {dbfs_error}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Directory creation failed: {e}\")\n",
    "    raise\n",
    "# Step 3: Initialize or get existing GX context\n",
    "print(\"\\nINITIALIZING GX CONTEXT\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "context = None\n",
    "context_creation_methods = [\n",
    "    (\"DBFS FileDataContext\", lambda: gx.get_context(project_root_dir=DBFS_GX_ROOT)),\n",
    "    (\"Ephemeral Context\", lambda: gx.get_context(mode=\"ephemeral\")),\n",
    "    (\"Default Context\", lambda: gx.get_context())\n",
    "]\n",
    "\n",
    "for method_name, method_func in context_creation_methods:\n",
    "    try:\n",
    "        \n",
    "        context = method_func()\n",
    "       \n",
    "        print(f\"Context type: {type(context).__name__}\")\n",
    "        \n",
    "        if method_name == \"DBFS FileDataContext\":\n",
    "            print(f\"Persistent storage: {DBFS_GX_ROOT}\")\n",
    "      \n",
    "        \n",
    "        break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{method_name} failed: {e}\")\n",
    "        continue\n",
    "\n",
    "if context is None:\n",
    "    raise RuntimeError(\"All context creation methods failed\")\n",
    "\n",
    "# Step 4: Context verification and configuration\n",
    "print(\"\\nCONTEXT VERIFICATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    datasources = context.list_datasources()\n",
    "   \n",
    "    \n",
    "    key_methods = ['add_datasource', 'get_datasource', 'list_datasources']\n",
    "    available_methods = [method for method in key_methods if hasattr(context, method)]\n",
    "    print(f\"Available methods: {len(available_methods)}/{len(key_methods)}\")\n",
    "    \n",
    "    print(\"\\nCONTEXT\")\n",
    "    print(f\"Type: {type(context).__name__}\")\n",
    "   \n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Context verification warning: {e}\")\n",
    "\n",
    "# Step 5: Export for use in other cells\n",
    "print(\"\\nEXPORTING CONTEXT\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "globals()['gx_context'] = context\n",
    "globals()['DBFS_GX_ROOT'] = DBFS_GX_ROOT\n",
    "\n",
    "print(\"Context exported as 'gx_context'\")\n",
    "print(\"DBFS path exported as 'DBFS_GX_ROOT'\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581bfb69-42b0-41aa-af35-0f1bc7efc951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   • Server: shell-31-eun-sq-odduxhlfvttsvnrucqpy.database.windows.net:1433\n   • Database: shell-31-eun-sqdb-edcruevdeslqjpdpwlcz\n   • Authentication: Active Directory Service Principal\n   • Client ID: AMSDQDB_...\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# step3_results = {\n",
    "#     \"status\": \"running\",\n",
    "#     \"data_loaded\": False,\n",
    "#     \"target_table\": \"DQ_LOGIC\",\n",
    "#     \"record_count\": 0,\n",
    "#     \"columns\": [],\n",
    "#     \"data_types\": {},\n",
    "#     \"sample_data\": [],\n",
    "#     \"ready_for_step4\": False,\n",
    "#     \"error_message\": None\n",
    "# }\n",
    "\n",
    "\n",
    "# # Use connection details from your working GX Helper v4\n",
    "# try:\n",
    "#     server = \"shell-31-eun-sq-odduxhlfvttsvnrucqpy.database.windows.net\"\n",
    "#     database = \"shell-31-eun-sqdb-edcruevdeslqjpdpwlcz\"\n",
    "    \n",
    "#     # Use the CORRECT secret keys from your working code\n",
    "#     username = dbutils.secrets.get(\"ADLSScope\", \"AzSQL-DBr-PROD\")\n",
    "#     password = dbutils.secrets.get(\"ADLSScope\", \"ADB-AzSQL-PROD-pwd\")\n",
    "    \n",
    "#     # Build JDBC connection components for use in data loading\n",
    "#     jdbc_url = f\"jdbc:sqlserver://{server}:1433;database={database};user={username};password={password};encrypt=true;trustServerCertificate=true;loginTimeout=30;\"\n",
    "    \n",
    "#     connection_properties = {\n",
    "#         \"user\": username,\n",
    "#         \"password\": password,\n",
    "#         \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "#     }\n",
    "    \n",
    "#     print(\"✅ Using SQL Server connection configuration\")\n",
    "#     print(f\"   • Server: {server}\")\n",
    "#     print(f\"   • Database: {database}\")\n",
    "#     print(f\"   • Username: {username}\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Failed to get connection details: {e}\")\n",
    "#     step3_results[\"error_message\"] = f\"Connection details failed: {e}\"\n",
    "#     step3_results[\"status\"] = \"error\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "step3_results = {\n",
    "    \"status\": \"running\",\n",
    "    \"data_loaded\": False,\n",
    "    \"target_table\": \"DQ_LOGIC\",\n",
    "    \"record_count\": 0,\n",
    "    \"columns\": [],\n",
    "    \"data_types\": {},\n",
    "    \"sample_data\": [],\n",
    "    \"ready_for_step4\": False,\n",
    "    \"error_message\": None\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    server = \"shell-31-eun-sq-odduxhlfvttsvnrucqpy.database.windows.net\"\n",
    "    port = 1433\n",
    "    database = \"shell-31-eun-sqdb-edcruevdeslqjpdpwlcz\"\n",
    "    \n",
    "\n",
    "    client_id = dbutils.secrets.get(scope=\"ADLSScope\", key=\"AzSQL-DBr-PROD\")\n",
    "    client_secret = dbutils.secrets.get(scope=\"ADLSScope\", key=\"ADB-AzSQL-PROD-pwd\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    jdbc_url = f\"\"\"jdbc:sqlserver://{server}:{port};database={database};authentication=ActiveDirectoryServicePrincipal;encrypt=true;hostNameInCertificate=*.database.windows.net;loginTimeout=30;AADSecurePrincipalId={client_id};AADSecurePrincipalSecret={client_secret}\"\"\"\n",
    "    \n",
    "    connection_properties = {\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "        \"authentication\": \"ActiveDirectoryServicePrincipal\",\n",
    "        \"AADSecurePrincipalId\": client_id,\n",
    "        \"AADSecurePrincipalSecret\": client_secret,\n",
    "        \"encrypt\": \"true\",\n",
    "        \"hostNameInCertificate\": \"*.database.windows.net\",\n",
    "        \"loginTimeout\": \"30\"\n",
    "    }\n",
    "    \n",
    "  \n",
    "    print(f\"   • Server: {server}:{port}\")\n",
    "    print(f\"   • Database: {database}\")\n",
    "    print(f\"   • Authentication: Active Directory Service Principal\")\n",
    "    print(f\"   • Client ID: {client_id[:8]}...\")  # Show only first 8 chars for security\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to get connection details: {e}\")\n",
    "    step3_results[\"error_message\"] = f\"Connection details failed: {e}\"\n",
    "    step3_results[\"status\"] = \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f6981cd-0ff5-4cff-8bc9-37fec794332a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA TARGET TABLE DATA LOADING\n--------------------------------------------------\n\uD83D\uDCE5 Loading data from dbo.DQ_LOGIC...\n✅ Connection successful! Found 358,170 rows in dbo.DQ_LOGIC\n✅ Data loaded successfully\n\uD83D\uDCCA Records: 1000\n\uD83D\uDCDD Columns: 9\n\n\uD83D\uDCCB COLUMN INFORMATION:\n  \uD83D\uDCCC id: int32\n  \uD83D\uDCCC Global_Rule_Id: int32\n  \uD83D\uDCCC HIERARCHY_ID: int32\n  \uD83D\uDCCC DQ_SQL: object\n  \uD83D\uDCCC ERROR_SQL: object\n  \uD83D\uDCCC RECORD_CREATE_DATE: object\n  \uD83D\uDCCC RECORD_CREATED_BY: object\n  \uD83D\uDCCC RECORD_UPDATE_DATE: object\n  \uD83D\uDCCC RECORD_UPDATED_BY: object\n\uD83D\uDCCB Columns: ['id', 'Global_Rule_Id', 'HIERARCHY_ID', 'DQ_SQL', 'ERROR_SQL', 'RECORD_CREATE_DATE', 'RECORD_CREATED_BY', 'RECORD_UPDATE_DATE', 'RECORD_UPDATED_BY']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TARGET TABLE DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "if step3_results[\"status\"] != \"error\":\n",
    "    print(f\"\\n\uD83D\uDCCA TARGET TABLE DATA LOADING\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    target_table = \"dbo.DQ_LOGIC\"\n",
    "    step3_results[\"target_table\"] = target_table\n",
    "    \n",
    "    try:\n",
    "        # Use the EXACT approach from your working GX Helper v4\n",
    "        print(f\"\uD83D\uDCE5 Loading data from {target_table}...\")\n",
    "        \n",
    "        # First, test connectivity with a count query (like your working v4)\n",
    "        test_df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", \"(SELECT COUNT(*) as test_count FROM dbo.DQ_LOGIC) as test\") \\\n",
    "            .load()\n",
    "        \n",
    "        test_count = test_df.collect()[0]['test_count']\n",
    "        print(f\"✅ Connection successful! Found {test_count:,} rows in dbo.DQ_LOGIC\")\n",
    "        \n",
    "        # Now load actual data using the working pattern\n",
    "        data_df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", \"(SELECT TOP 1000 * FROM dbo.DQ_LOGIC) as validation_data\") \\\n",
    "            .load()\n",
    "        \n",
    "        # Convert to Pandas for easier GX integration (like your working v4)\n",
    "        df = data_df.toPandas()\n",
    "        \n",
    "        # Get basic statistics\n",
    "        record_count = len(df)\n",
    "        columns = list(df.columns)\n",
    "        \n",
    "        step3_results[\"record_count\"] = record_count\n",
    "        step3_results[\"columns\"] = columns\n",
    "        step3_results[\"data_loaded\"] = True\n",
    "        \n",
    "        print(f\"✅ Data loaded successfully\")\n",
    "        print(f\"\uD83D\uDCCA Records: {record_count}\")\n",
    "        print(f\"\uD83D\uDCDD Columns: {len(columns)}\")\n",
    "        \n",
    "        # Get column information from Pandas DataFrame\n",
    "        print(f\"\\n\uD83D\uDCCB COLUMN INFORMATION:\")\n",
    "        \n",
    "        # Get data types from the Pandas DataFrame\n",
    "        for col_name in df.columns:\n",
    "            col_type = str(df[col_name].dtype)\n",
    "            step3_results[\"data_types\"][col_name] = {\n",
    "                \"pandas_type\": col_type,\n",
    "                \"has_nulls\": df[col_name].isnull().any(),\n",
    "                \"null_count\": df[col_name].isnull().sum()\n",
    "            }\n",
    "            print(f\"  \uD83D\uDCCC {col_name}: {col_type}\")\n",
    "        \n",
    "        print(f\"\uD83D\uDCCB Columns: {list(df.columns)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data loading failed: {e}\")\n",
    "        step3_results[\"data_loaded\"] = False\n",
    "        step3_results[\"error_message\"] = f\"Data loading failed: {e}\"\n",
    "        step3_results[\"status\"] = \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75701d5a-875c-43ff-867e-5c71e673f064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TARGET TABLE DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "if step3_results[\"status\"] != \"error\":\n",
    "\n",
    "    \n",
    "    target_table = \"dbo.DQ_LOGIC\"\n",
    "    step3_results[\"target_table\"] = target_table\n",
    "    \n",
    "    try:\n",
    "\n",
    "        \n",
    "        # First, test connectivity with a count query (like your working v4)\n",
    "        test_df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", \"(SELECT COUNT(*) as test_count FROM dbo.DQ_LOGIC) as test\") \\\n",
    "            .load()\n",
    "        \n",
    "        test_count = test_df.collect()[0]['test_count']\n",
    " \n",
    "\n",
    "        data_df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", \"(SELECT TOP 1000 * FROM dbo.DQ_LOGIC) as validation_data\") \\\n",
    "            .load()\n",
    "        \n",
    "\n",
    "        df = data_df.toPandas()\n",
    "        \n",
    "\n",
    "        record_count = len(df)\n",
    "        columns = list(df.columns)\n",
    "        \n",
    "        step3_results[\"record_count\"] = record_count\n",
    "        step3_results[\"columns\"] = columns\n",
    "        step3_results[\"data_loaded\"] = True\n",
    "        \n",
    "\n",
    "\n",
    "        print(f\"COLUMN INFORMATION:\")\n",
    "        \n",
    "\n",
    "        for col_name in df.columns:\n",
    "            col_type = str(df[col_name].dtype)\n",
    "            step3_results[\"data_types\"][col_name] = {\n",
    "                \"pandas_type\": col_type,\n",
    "                \"has_nulls\": df[col_name].isnull().any(),\n",
    "                \"null_count\": df[col_name].isnull().sum()\n",
    "            }\n",
    "            print(f\"{col_name}: {col_type}\")\n",
    "        \n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Data loading failed: {e}\")\n",
    "        step3_results[\"data_loaded\"] = False\n",
    "        step3_results[\"error_message\"] = f\"Data loading failed: {e}\"\n",
    "        step3_results[\"status\"] = \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16326c4-00c0-4bc9-a986-b38225df17ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PROFILING & VALIDATION PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "if step3_results[\"data_loaded\"]:\n",
    "    print(f\"\\n\uD83D\uDCC8 DATA PROFILING & VALIDATION PREPARATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Get sample data for inspection (df is now a Pandas DataFrame)\n",
    "        sample_data = df.head(5).to_dict('records')\n",
    "        step3_results[\"sample_data\"] = sample_data\n",
    "        \n",
    "        print(f\"\uD83D\uDCDD SAMPLE DATA (First 5 rows):\")\n",
    "        for i, row in enumerate(sample_data, 1):\n",
    "            # Show first 3 columns from the dictionary\n",
    "            first_three = dict(list(row.items())[:3])\n",
    "            print(f\"  Row {i}: {first_three}...\")\n",
    "        \n",
    "        # Check for target columns mentioned in requirements\n",
    "        target_columns = [\"HIERARCHY_ID\", \"RECORD_CREATE_DATE\"]\n",
    "        missing_columns = []\n",
    "        present_columns = []\n",
    "        \n",
    "        print(f\"\\n\uD83C\uDFAF TARGET COLUMNS CHECK:\")\n",
    "        for col in target_columns:\n",
    "            if col in columns:\n",
    "                present_columns.append(col)\n",
    "                print(f\"  ✅ {col} - Present\")\n",
    "            else:\n",
    "                missing_columns.append(col)\n",
    "                print(f\"  ❌ {col} - Missing\")\n",
    "        \n",
    "        step3_results[\"target_columns\"] = {\n",
    "            \"required\": target_columns,\n",
    "            \"present\": present_columns,\n",
    "            \"missing\": missing_columns\n",
    "        }\n",
    "        \n",
    "        # Basic data quality checks\n",
    "        print(f\"\\n\uD83D\uDCCA BASIC DATA QUALITY CHECKS:\")\n",
    "        \n",
    "        # Check for null values in key columns\n",
    "        if present_columns:\n",
    "            for col in present_columns:\n",
    "                # Use Pandas methods since df is now a Pandas DataFrame\n",
    "                null_count = int(df[col].isnull().sum())\n",
    "                null_percentage = (null_count / record_count) * 100 if record_count > 0 else 0\n",
    "                print(f\"  \uD83D\uDCCC {col}: {null_count} nulls ({null_percentage:.1f}%)\")\n",
    "        \n",
    "        # Check data freshness if RECORD_CREATE_DATE exists\n",
    "        if \"RECORD_CREATE_DATE\" in present_columns:\n",
    "            try:\n",
    "                # Use Pandas methods for date analysis\n",
    "                min_date = df[\"RECORD_CREATE_DATE\"].min()\n",
    "                max_date = df[\"RECORD_CREATE_DATE\"].max()\n",
    "                \n",
    "                # Convert to strings for JSON serialization\n",
    "                min_date_str = str(min_date)\n",
    "                max_date_str = str(max_date)\n",
    "                \n",
    "                print(f\"  \uD83D\uDCC5 Date range: {min_date_str} to {max_date_str}\")\n",
    "                step3_results[\"date_range\"] = {\n",
    "                    \"min_date\": min_date_str,\n",
    "                    \"max_date\": max_date_str\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️  Could not analyze date range: {e}\")\n",
    "        \n",
    "        # DataFrame is already Pandas, so no need to convert again\n",
    "        step3_results[\"pandas_sample_size\"] = len(df)\n",
    "        print(f\"\uD83D\uDCCA Pandas DataFrame available: {len(df)} rows\")\n",
    "        \n",
    "        # Store DataFrame reference for next steps\n",
    "        step3_results[\"dataframe_available\"] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data profiling failed: {e}\")\n",
    "        step3_results[\"error_message\"] = f\"Data profiling failed: {e}\"\n",
    "\n",
    "# Final readiness assessment - ensure all values are Python native types\n",
    "ready_for_step4 = bool(\n",
    "    step3_results[\"data_loaded\"] and\n",
    "    step3_results[\"record_count\"] > 0\n",
    ")\n",
    "\n",
    "step3_results[\"ready_for_step4\"] = ready_for_step4\n",
    "step3_results[\"status\"] = \"success\" if ready_for_step4 else \"error\"\n",
    "\n",
    "# Convert any remaining numpy types to Python native types\n",
    "def convert_to_native_types(obj):\n",
    "    \"\"\"Convert numpy types to native Python types for JSON serialization\"\"\"\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_native_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native_types(v) for v in obj]\n",
    "    elif isinstance(obj, (datetime.date, datetime.datetime)):\n",
    "        return str(obj)  # Convert date/datetime objects to strings\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    elif hasattr(obj, 'dtype'):  # Other numpy types\n",
    "        if 'bool' in str(obj.dtype):\n",
    "            return bool(obj)\n",
    "        elif 'int' in str(obj.dtype):\n",
    "            return int(obj)\n",
    "        elif 'float' in str(obj.dtype):\n",
    "            return float(obj)\n",
    "        else:\n",
    "            return str(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Clean the results dictionary\n",
    "step3_results = convert_to_native_types(step3_results)\n",
    "\n",
    "# Return results for orchestrator\n",
    "dbutils.notebook.exit(json.dumps(step3_results))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "step3_sql_connection_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}