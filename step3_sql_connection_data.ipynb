{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6b712d2-14d1-4f79-a82e-be8d2a1e59f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 3: SQL connectivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db288160-4a64-483b-9735-007a81106aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: great_expectations in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (1.5.7)\nRequirement already satisfied: sqlalchemy in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (2.0.42)\nRequirement already satisfied: pyodbc in /databricks/python3/lib/python3.11/site-packages (4.0.39)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.11/site-packages (1.5.3)\nRequirement already satisfied: altair<5.0.0,>=4.2.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (4.2.2)\nRequirement already satisfied: cryptography>=3.2 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (41.0.3)\nRequirement already satisfied: jinja2>=3 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (3.1.6)\nRequirement already satisfied: jsonschema>=2.5.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (4.25.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.7.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (3.26.1)\nRequirement already satisfied: mistune>=0.8.4 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (3.1.3)\nRequirement already satisfied: numpy>=1.22.4 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (1.23.5)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (23.2)\nRequirement already satisfied: posthog>3 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (6.3.2)\nRequirement already satisfied: pydantic>=1.10.7 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (2.11.7)\nRequirement already satisfied: pyparsing>=2.4 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (2.8.2)\nRequirement already satisfied: requests>=2.20 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (2.31.0)\nRequirement already satisfied: ruamel.yaml>=0.16 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (0.18.14)\nRequirement already satisfied: scipy>=1.6.0 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (1.11.1)\nRequirement already satisfied: tqdm>=4.59.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.1.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (4.14.1)\nRequirement already satisfied: tzlocal>=1.2 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (5.3.1)\nRequirement already satisfied: greenlet>=1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from sqlalchemy) (3.2.3)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas) (2022.7)\nRequirement already satisfied: entrypoints in /databricks/python3/lib/python3.11/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (0.4)\nRequirement already satisfied: toolz in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (1.0.0)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=3.2->great_expectations) (1.15.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jinja2>=3->great_expectations) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (0.26.0)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from posthog>3->great_expectations) (1.16.0)\nRequirement already satisfied: backoff>=1.10.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from posthog>3->great_expectations) (2.2.1)\nRequirement already satisfied: distro>=1.5.0 in /usr/lib/python3/dist-packages (from posthog>3->great_expectations) (1.7.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic>=1.10.7->great_expectations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic>=1.10.7->great_expectations) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic>=1.10.7->great_expectations) (0.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (2023.7.22)\nRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from ruamel.yaml>=0.16->great_expectations) (0.2.12)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.2->great_expectations) (2.21)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install great_expectations sqlalchemy pyodbc pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57ae9fc7-63d2-4f3e-ab5b-82ec9149eeeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "is_databricks = False\n",
    "try:\n",
    "    dbutils.fs.ls('/')\n",
    "    is_databricks = True\n",
    "\n",
    "except NameError:\n",
    "\n",
    "    raise EnvironmentError(\"This notebook requires Databricks environment\")\n",
    "\n",
    "\n",
    "\n",
    "gx_available = False\n",
    "try:\n",
    "    import great_expectations as gx\n",
    "   \n",
    "    gx_available = True\n",
    "except ImportError:\n",
    "    print()\n",
    "\n",
    "\n",
    "if not gx_available:\n",
    "\n",
    "    \n",
    "   \n",
    "    %pip install great-expectations[sql,azure,databricks,postgresql,snowflake] --upgrade --quiet\n",
    "    \n",
    "   \n",
    "    %pip install sqlalchemy>=2.0.0 pandas>=1.5.0 plotly pyodbc --upgrade --quiet\n",
    "    \n",
    "\n",
    "    dbutils.library.restartPython()\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    import great_expectations as gx\n",
    "    import pandas as pd\n",
    "    import sqlalchemy\n",
    "    \n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"{e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c180e9-8bea-4685-8fe1-29ca710ff429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load Step 2 results: name 'json' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    step2_results = dbutils.notebook.run(\"./step2_gx_context_setup\", 0)\n",
    "    step2_data = json.loads(step2_results)\n",
    "    \n",
    "    if step2_data.get(\"context_created\"):\n",
    "        print(f\"Step 2 context available: {step2_data.get('context_type')}\")\n",
    "        discovered_methods = step2_data.get(\"context_methods\", [])\n",
    "        method_categories = step2_data.get(\"method_categories\", {})\n",
    "        print(f\"Total methods discovered: {len(discovered_methods)}\")\n",
    "    else:\n",
    "        print(f\"Step 2 context not available, creating fresh context\")\n",
    "        discovered_methods = []\n",
    "        method_categories = {}\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not load Step 2 results: {e}\")\n",
    "    discovered_methods = []\n",
    "    method_categories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581bfb69-42b0-41aa-af35-0f1bc7efc951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table schema loaded: 13 columns\nAll critical columns present: ['WellCode', 'Process', 'record_create_date']\nTarget table: `aueasset_edp-unitycatalog-tst`.`aca`.`dq_error_result`\nExecuting Unity Catalog query...\nShape: 1,000 rows × 10 columns\nColumns: ['WellCode', 'Process', 'record_create_date', 'dq_date', 'rule_name', 'rule_status', 'severity', 'sum_frs_rows', 'sum_frs_covered', 'sum_frs_not_covered']\nData Quality Summary:\n  • Unique Wells: 508\n  • Unique Processes: 2\n  • Date Range: 2025-07-01 08:49:09.466000 to 2025-07-01 08:49:09.466000\n  • Missing Values: 0\n    WellCode: 0 nulls (0.0%)\n    Process: 0 nulls (0.0%)\n    record_create_date: 0 nulls (0.0%)\n\n{\n  \"status\": \"success\",\n  \"unity_catalog_connection\": true,\n  \"data_loaded\": true,\n  \"target_table\": \"`aueasset_edp-unitycatalog-tst`.`aca`.`dq_error_result`\",\n  \"record_count\": 1000,\n  \"columns\": [\n    \"WellCode\",\n    \"Process\",\n    \"record_create_date\",\n    \"dq_date\",\n    \"rule_name\",\n    \"rule_status\",\n    \"severity\",\n    \"sum_frs_rows\",\n    \"sum_frs_covered\",\n    \"sum_frs_not_covered\"\n  ],\n  \"data_types\": {\n    \"WellCode\": \"string\",\n    \"Process\": \"string\",\n    \"sum_frs_rows\": \"float\",\n    \"sum_frs_covered\": \"float\",\n    \"sum_frs_not_covered\": \"float\",\n    \"rule_status\": \"string\",\n    \"dq_date\": \"date\",\n    \"rule_name\": \"string\",\n    \"rule_description\": \"string\",\n    \"rule_category\": \"string\",\n    \"severity\": \"string\",\n    \"record_create_date\": \"timestamp\",\n    \"record_created_by\": \"string\"\n  },\n  \"ready_for_step4\": true,\n  \"error_message\": null,\n  \"connection_method\": \"spark_sql\",\n  \"critical_columns\": [\n    \"WellCode\",\n    \"Process\",\n    \"record_create_date\"\n  ],\n  \"critical_column_analysis\": {\n    \"WellCode\": {\n      \"exists\": true,\n      \"null_count\": 0,\n      \"null_percentage\": 0.0,\n      \"unique_count\": 508,\n      \"data_type\": \"object\"\n    },\n    \"Process\": {\n      \"exists\": true,\n      \"null_count\": 0,\n      \"null_percentage\": 0.0,\n      \"unique_count\": 2,\n      \"data_type\": \"object\"\n    },\n    \"record_create_date\": {\n      \"exists\": true,\n      \"null_count\": 0,\n      \"null_percentage\": 0.0,\n      \"unique_count\": 1,\n      \"data_type\": \"datetime64[ns]\"\n    }\n  }\n}\n\n============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, module='pyspark')\n",
    "\n",
    "step3_results = {\n",
    "    \"status\": \"running\",\n",
    "    \"unity_catalog_connection\": False,\n",
    "    \"data_loaded\": False,\n",
    "    \"target_table\": None,\n",
    "    \"record_count\": 0,\n",
    "    \"columns\": [],\n",
    "    \"data_types\": {},\n",
    "    \"sample_data\": [],\n",
    "    \"ready_for_step4\": False,\n",
    "    \"error_message\": None,\n",
    "    \"connection_method\": \"spark_sql\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "UC_CONFIG = {\n",
    "    \"catalog\": \"aueasset_edp-unitycatalog-tst\",\n",
    "    \"schema\": \"aca\", \n",
    "    \"table\": \"dq_error_result\",\n",
    "    \"full_table_name\": \"`aueasset_edp-unitycatalog-tst`.`aca`.`dq_error_result`\"\n",
    "}\n",
    "\n",
    "\n",
    "CRITICAL_COLUMNS = [\"WellCode\", \"Process\", \"record_create_date\"]\n",
    "MAX_ROWS_TO_LOAD = 1000  \n",
    "\n",
    "\n",
    "\n",
    "step3_results[\"target_table\"] = UC_CONFIG['full_table_name']\n",
    "step3_results[\"critical_columns\"] = CRITICAL_COLUMNS\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    spark_session = spark  \n",
    "   \n",
    "    \n",
    "\n",
    "    test_query = f\"DESCRIBE TABLE {UC_CONFIG['full_table_name']}\"\n",
    "\n",
    "    \n",
    "\n",
    "    schema_df = spark.sql(test_query)\n",
    "    schema_info = schema_df.collect()\n",
    "    \n",
    "    column_names = [row['col_name'] for row in schema_info if row['col_name'] not in ['', None]]\n",
    "    column_types = [row['data_type'] for row in schema_info if row['col_name'] not in ['', None]]\n",
    "    \n",
    "\n",
    "    print(f\"Table schema loaded: {len(column_names)} columns\")\n",
    "    \n",
    "    step3_results[\"unity_catalog_connection\"] = True\n",
    "    step3_results[\"columns\"] = column_names\n",
    "    step3_results[\"data_types\"] = dict(zip(column_names, column_types))\n",
    "    \n",
    "\n",
    "    missing_critical = [col for col in CRITICAL_COLUMNS if col not in column_names]\n",
    "    if missing_critical:\n",
    "        print(f\"Missing critical columns: {missing_critical}\")\n",
    "    else:\n",
    "        print(f\"All critical columns present: {CRITICAL_COLUMNS}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Unity Catalog connection failed: {e}\")\n",
    "    step3_results[\"error_message\"] = f\"Unity Catalog connection failed: {e}\"\n",
    "    step3_results[\"status\"] = \"error\"\n",
    "    raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_unity_catalog_data():\n",
    "    \"\"\"\n",
    "    Load data from Unity Catalog using Spark SQL (native Databricks approach)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Target table: {UC_CONFIG['full_table_name']}\")\n",
    "        \n",
    "\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            WellCode,\n",
    "            Process,\n",
    "            record_create_date,\n",
    "            dq_date,\n",
    "            rule_name,\n",
    "            rule_status,\n",
    "            severity,\n",
    "            sum_frs_rows,\n",
    "            sum_frs_covered,\n",
    "            sum_frs_not_covered\n",
    "        FROM {UC_CONFIG['full_table_name']}\n",
    "        ORDER BY record_create_date DESC\n",
    "        LIMIT {MAX_ROWS_TO_LOAD}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Executing Unity Catalog query...\")\n",
    "        spark_df = spark.sql(query)\n",
    "        \n",
    "\n",
    "        unity_df = spark_df.toPandas()\n",
    "        \n",
    "\n",
    "        print(f\"Shape: {unity_df.shape[0]:,} rows × {unity_df.shape[1]} columns\")\n",
    "        print(f\"Columns: {list(unity_df.columns)}\")\n",
    "        \n",
    "\n",
    "        print(f\"Data Quality Summary:\")\n",
    "        if 'WellCode' in unity_df.columns:\n",
    "            print(f\"  • Unique Wells: {unity_df['WellCode'].nunique():,}\")\n",
    "        if 'Process' in unity_df.columns:\n",
    "            print(f\"  • Unique Processes: {unity_df['Process'].nunique()}\")\n",
    "        if 'record_create_date' in unity_df.columns:\n",
    "            print(f\"  • Date Range: {unity_df['record_create_date'].min()} to {unity_df['record_create_date'].max()}\")\n",
    "        \n",
    "        null_count = unity_df.isnull().sum().sum()\n",
    "        print(f\"  • Missing Values: {null_count:,}\")\n",
    "        \n",
    "        return unity_df, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Data loading failed: {e}\")\n",
    "        return None, False\n",
    "\n",
    "\n",
    "unity_df, data_loading_success = load_unity_catalog_data()\n",
    "\n",
    "if data_loading_success and unity_df is not None:\n",
    "    step3_results[\"data_loaded\"] = True\n",
    "    step3_results[\"record_count\"] = len(unity_df)\n",
    "    step3_results[\"columns\"] = list(unity_df.columns)\n",
    "    \n",
    "\n",
    "    step3_results[\"sample_data\"] = unity_df.head(5).to_dict('records')\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    critical_column_analysis = {}\n",
    "    for col in CRITICAL_COLUMNS:\n",
    "        if col in unity_df.columns:\n",
    "            null_count = unity_df[col].isnull().sum()\n",
    "            null_percentage = (null_count / len(unity_df)) * 100\n",
    "            unique_count = unity_df[col].nunique()\n",
    "            \n",
    "            critical_column_analysis[col] = {\n",
    "                \"exists\": True,\n",
    "                \"null_count\": int(null_count),\n",
    "                \"null_percentage\": float(null_percentage),\n",
    "                \"unique_count\": int(unique_count),\n",
    "                \"data_type\": str(unity_df[col].dtype)\n",
    "            }\n",
    "            \n",
    "            status = \"\" if null_count == 0 else \"\"\n",
    "            print(f\"   {status} {col}: {null_count} nulls ({null_percentage:.1f}%)\")\n",
    "        else:\n",
    "            critical_column_analysis[col] = {\n",
    "                \"exists\": False,\n",
    "                \"null_count\": None,\n",
    "                \"null_percentage\": None,\n",
    "                \"unique_count\": None,\n",
    "                \"data_type\": None\n",
    "            }\n",
    "            print(\"\")\n",
    "    \n",
    "    step3_results[\"critical_column_analysis\"] = critical_column_analysis\n",
    "    \n",
    "\n",
    "    globals()['unity_df'] = unity_df\n",
    "    globals()['df'] = unity_df \n",
    "    globals()['UC_CONFIG'] = UC_CONFIG\n",
    "    globals()['CRITICAL_COLUMNS'] = CRITICAL_COLUMNS\n",
    "    globals()['unity_data_loaded'] = True\n",
    "    globals()['data_available'] = True\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "else:\n",
    "    step3_results[\"error_message\"] = \"Data loading failed\"\n",
    "    step3_results[\"status\"] = \"error\"\n",
    "    \n",
    "\n",
    "    globals()['unity_df'] = None\n",
    "    globals()['df'] = None\n",
    "    globals()['unity_data_loaded'] = False\n",
    "    globals()['data_available'] = False\n",
    "\n",
    "\n",
    "ready_for_step4 = (\n",
    "    step3_results[\"unity_catalog_connection\"] and\n",
    "    step3_results[\"data_loaded\"] and\n",
    "    step3_results[\"record_count\"] > 0\n",
    ")\n",
    "\n",
    "step3_results[\"ready_for_step4\"] = ready_for_step4\n",
    "step3_results[\"status\"] = \"success\" if ready_for_step4 else \"error\"\n",
    "\n",
    "\n",
    "\n",
    "if ready_for_step4:\n",
    "    print(f\"\")\n",
    "\n",
    "else:\n",
    "\n",
    "    if step3_results.get(\"error_message\"):\n",
    "        print(f\"   • {step3_results['error_message']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(json.dumps({k: v for k, v in step3_results.items() if k not in ['sample_data']}, indent=2))\n",
    "\n",
    "\n",
    "\n",
    "if ready_for_step4:\n",
    "    print(\"\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75701d5a-875c-43ff-867e-5c71e673f064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_for_json(obj):\n",
    "    \"\"\"\n",
    "    Clean object for JSON serialization by converting non-serializable types\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        return {key: clean_for_json(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_for_json(item) for item in obj]\n",
    "    elif isinstance(obj, (pd.Timestamp, datetime.datetime, datetime.date, np.datetime64)):\n",
    "        return obj.isoformat() if hasattr(obj, 'isoformat') else str(obj)\n",
    "    elif isinstance(obj, (pd.Series, pd.DataFrame)):\n",
    "        return obj.to_dict() if hasattr(obj, 'to_dict') else str(obj)\n",
    "    elif hasattr(obj, 'item'):  # numpy types\n",
    "        return obj.item()\n",
    "    elif isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.bool_, bool)):\n",
    "        return bool(obj)\n",
    "    elif not isinstance(obj, (str, int, float, bool, type(None))):\n",
    "        return str(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Clean and return results for notebook orchestration\n",
    "step3_results = clean_for_json(step3_results)\n",
    "dbutils.notebook.exit(json.dumps(step3_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c920d773-fac5-4b01-bd62-2fa937fb5ba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(f\"SAVING STEP 3 RESULTS TO DBFS\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# try:\n",
    "#     import os\n",
    "#     import json\n",
    "#     from datetime import datetime\n",
    "    \n",
    "#     # Create results directory if it doesn't exist\n",
    "#     results_dir = \"/dbfs/FileStore/great_expectations/step_results/\"\n",
    "#     os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "#     # Save step3 results with timestamp\n",
    "#     result_file = f\"{results_dir}step3_results.json\"\n",
    "    \n",
    "#     # Add timestamp to results for tracking\n",
    "#     timestamped_results = {\n",
    "#         **step3_results,\n",
    "#         \"saved_timestamp\": datetime.now().isoformat(),\n",
    "#         \"step_name\": \"step3\"\n",
    "#     }\n",
    "    \n",
    "#     with open(result_file, 'w') as f:\n",
    "#         json.dump(timestamped_results, f, indent=2, default=str)\n",
    "        \n",
    "#     print(f\"Step 3 results saved to: {result_file}\")\n",
    "#     print(f\"Records: {step3_results.get('record_count', 0)} loaded\")\n",
    "#     print(f\"Columns: {len(step3_results.get('columns', []))} discovered\")\n",
    "#     print(f\"Status: {step3_results.get('status', 'unknown')}\")\n",
    "#     print(f\"Available for downstream steps\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"Could not save step3 results: {e}\")\n",
    "#     print(f\"Results still available via notebook return value\")\n",
    "\n",
    "# print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "step3_sql_connection_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}