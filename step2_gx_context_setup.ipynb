{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21637be7-1e48-4ffa-aac8-c6d5d64b21dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: GX Context Setup & API Discovery\n",
    "\n",
    "\n",
    "- step1 shows installation\n",
    "- step2 python restart\n",
    "- step3 Context check\n",
    "- total available Methods by GX\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba3722b-c80a-462d-8a19-e9f1a84cb1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: great_expectations in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (1.5.7)\nRequirement already satisfied: sqlalchemy in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (2.0.42)\nRequirement already satisfied: pyodbc in /databricks/python3/lib/python3.11/site-packages (4.0.39)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.11/site-packages (1.5.3)\nRequirement already satisfied: altair<5.0.0,>=4.2.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (4.2.2)\nRequirement already satisfied: cryptography>=3.2 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (41.0.3)\nRequirement already satisfied: jinja2>=3 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (3.1.6)\nRequirement already satisfied: jsonschema>=2.5.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (4.25.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.7.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (3.26.1)\nRequirement already satisfied: mistune>=0.8.4 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (3.1.3)\nRequirement already satisfied: numpy>=1.22.4 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (1.23.5)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (23.2)\nRequirement already satisfied: posthog>3 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (6.3.2)\nRequirement already satisfied: pydantic>=1.10.7 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (2.11.7)\nRequirement already satisfied: pyparsing>=2.4 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (2.8.2)\nRequirement already satisfied: requests>=2.20 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (2.31.0)\nRequirement already satisfied: ruamel.yaml>=0.16 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (0.18.14)\nRequirement already satisfied: scipy>=1.6.0 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (1.11.1)\nRequirement already satisfied: tqdm>=4.59.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.1.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (4.14.1)\nRequirement already satisfied: tzlocal>=1.2 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from great_expectations) (5.3.1)\nRequirement already satisfied: greenlet>=1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from sqlalchemy) (3.2.3)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas) (2022.7)\nRequirement already satisfied: entrypoints in /databricks/python3/lib/python3.11/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (0.4)\nRequirement already satisfied: toolz in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (1.0.0)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=3.2->great_expectations) (1.15.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jinja2>=3->great_expectations) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (0.26.0)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from posthog>3->great_expectations) (1.16.0)\nRequirement already satisfied: backoff>=1.10.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from posthog>3->great_expectations) (2.2.1)\nRequirement already satisfied: distro>=1.5.0 in /usr/lib/python3/dist-packages (from posthog>3->great_expectations) (1.7.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic>=1.10.7->great_expectations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic>=1.10.7->great_expectations) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic>=1.10.7->great_expectations) (0.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (2023.7.22)\nRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from ruamel.yaml>=0.16->great_expectations) (0.2.12)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.2->great_expectations) (2.21)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Run this in a separate cell before restarting\n",
    "%pip install great_expectations sqlalchemy pyodbc pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "330f5f2e-64f2-49e6-9467-63db30a79e23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.library.restartPython()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef9608ce-84a1-4c8f-a361-6172790a75db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GX Version: 1.5.7\nGX CLASS STRUCTURE \n--------------------------------------------------\nFileDataContext available\nEphemeralDataContext available\nCloudDataContext available\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import great_expectations as gx\n",
    "\n",
    "\n",
    "step2_results = {\n",
    "    \"status\": \"running\",\n",
    "    \"gx_version\": gx.__version__,\n",
    "    \"context_type\": None,\n",
    "    \"context_created\": False,\n",
    "    \"context_methods\": [],\n",
    "    \"datasource_methods\": [],\n",
    "    \"context_config\": {},\n",
    "    \"ready_for_step3\": False,\n",
    "    \"error_message\": None\n",
    "}\n",
    "\n",
    "print(f\"GX Version: {gx.__version__}\")\n",
    "\n",
    "try:\n",
    "\n",
    "    print(f\"GX CLASS STRUCTURE \")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check available context classes\n",
    "    context_classes = {}\n",
    "    \n",
    "    # FileDataContext\n",
    "    try:\n",
    "        from great_expectations.data_context import FileDataContext\n",
    "        context_classes[\"FileDataContext\"] = True\n",
    "        print(\"FileDataContext available\")\n",
    "    except ImportError:\n",
    "        context_classes[\"FileDataContext\"] = False\n",
    "        print(\"FileDataContext not available\")\n",
    "    \n",
    "    # EphemeralDataContext  \n",
    "    try:\n",
    "        from great_expectations.data_context import EphemeralDataContext\n",
    "        context_classes[\"EphemeralDataContext\"] = True\n",
    "        print(\"EphemeralDataContext available\")\n",
    "    except ImportError:\n",
    "        context_classes[\"EphemeralDataContext\"] = False\n",
    "        print(\"EphemeralDataContext not available\")\n",
    "    \n",
    "    # CloudDataContext\n",
    "    try:\n",
    "        from great_expectations.data_context import CloudDataContext\n",
    "        context_classes[\"CloudDataContext\"] = True\n",
    "        print(\"CloudDataContext available\")\n",
    "    except ImportError:\n",
    "        context_classes[\"CloudDataContext\"] = False\n",
    "        print(\"CloudDataContext not available\")\n",
    "    \n",
    "    step2_results[\"available_classes\"] = context_classes\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in class structure analysis: {e}\")\n",
    "    step2_results[\"error_message\"] = f\"Class analysis failed: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9e7f530-8e7b-49ca-9a02-548933e99b1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gx.get_context\n   gx.DataContext\n   gx.EphemeralDataContext\n   gx.FileDataContext\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: GX Available Context Classes\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "step2_results = {\n",
    "    \"status\": \"running\",\n",
    "    \"gx_version\": None,\n",
    "    \"context_type\": None,\n",
    "    \"context_created\": False,\n",
    "    \"context_methods\": [],\n",
    "    \"datasource_methods\": [],\n",
    "    \"context_config\": {},\n",
    "    \"ready_for_step3\": False,\n",
    "    \"error_message\": None,\n",
    "    \"import_retry_attempted\": False\n",
    "}\n",
    "\n",
    "\n",
    "gx = None\n",
    "import_successful = False\n",
    "\n",
    "\n",
    "try:\n",
    "    import great_expectations as gx\n",
    "    import_successful = True\n",
    "\n",
    "    step2_results[\"gx_version\"] = gx.__version__\n",
    "except ImportError as e:\n",
    " \n",
    "    step2_results[\"error_message\"] = f\"Import failed: {e}\"\n",
    "\n",
    "\n",
    "if not import_successful:\n",
    "\n",
    "    \n",
    "    try:\n",
    "       \n",
    "        import subprocess\n",
    "        result = subprocess.run([\n",
    "            sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"great-expectations[sql]\"\n",
    "        ], capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            \n",
    "            \n",
    "\n",
    "            if 'great_expectations' in sys.modules:\n",
    "                importlib.reload(sys.modules['great_expectations'])\n",
    "            \n",
    "  \n",
    "            import great_expectations as gx\n",
    "            import_successful = True\n",
    "            step2_results[\"import_retry_attempted\"] = True\n",
    "\n",
    "            step2_results[\"gx_version\"] = gx.__version__\n",
    "            \n",
    "        else:\n",
    "\n",
    "            step2_results[\"error_message\"] = f\"Installation failed: {result.stderr}\"\n",
    "            \n",
    "    except Exception as retry_error:\n",
    "\n",
    "        step2_results[\"error_message\"] = f\"Retry failed: {retry_error}\"\n",
    "\n",
    "\n",
    "if not import_successful or gx is None:\n",
    "\n",
    "    \n",
    "    step2_results[\"status\"] = \"error\"\n",
    "    raise RuntimeError(\"'great_expectations' is not available. Please restart the kernel after completing Step 1.\")\n",
    "\n",
    "\n",
    "if import_successful and gx:\n",
    "    try:\n",
    "       \n",
    "       \n",
    "        context_classes = {\n",
    "            'get_context': hasattr(gx, 'get_context'),\n",
    "            'DataContext': hasattr(gx, 'DataContext'),\n",
    "            'EphemeralDataContext': hasattr(gx, 'EphemeralDataContext'),\n",
    "            'FileDataContext': hasattr(gx, 'FileDataContext')\n",
    "        }\n",
    "        \n",
    "        \n",
    "        for class_name, available in context_classes.items():\n",
    "            status = \"\" if available else \"\"\n",
    "            print(f\"  {status} gx.{class_name}\")\n",
    "        \n",
    "        step2_results[\"context_classes\"] = context_classes\n",
    "        \n",
    "\n",
    "        if context_classes.get('get_context', False):\n",
    "\n",
    "            step2_results[\"modern_api\"] = True\n",
    "        else:\n",
    "\n",
    "            step2_results[\"modern_api\"] = False\n",
    "            \n",
    "\n",
    "        \n",
    "    except Exception as analysis_error:\n",
    "        \n",
    "        step2_results[\"error_message\"] = f\"Analysis failed: {analysis_error}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00d8d99-8bdd-4f4e-a684-bf57ed0a9aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBFS directory created/verified: /dbfs/FileStore/great_expectations\nDBFS-based context created successfully\nContext location: /dbfs/FileStore/great_expectations\nContext created: <class 'great_expectations.data_context.data_context.file_data_context.FileDataContext'>\nContext type: FileDataContext\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import great_expectations as gx\n",
    "\n",
    "# Check if running in Databricks\n",
    "try:\n",
    "    dbutils.fs.ls('/')\n",
    "    is_databricks = True\n",
    "except Exception:\n",
    "    is_databricks = False\n",
    "\n",
    "context = None\n",
    "\n",
    "if is_databricks:\n",
    "    dbfs_gx_path = \"/dbfs/FileStore/great_expectations\"\n",
    "    os.makedirs(dbfs_gx_path, exist_ok=True)\n",
    "    print(f\"DBFS directory created/verified: {dbfs_gx_path}\")\n",
    "\n",
    "    context = gx.get_context(project_root_dir=dbfs_gx_path)\n",
    "    print(\"DBFS-based context created successfully\")\n",
    "    print(f\"Context location: {dbfs_gx_path}\")\n",
    "\n",
    "if context is not None:\n",
    "    print(f\"Context created: {type(context)}\")\n",
    "    print(f\"Context type: {type(context).__name__}\")\n",
    "    \n",
    "    # Update step2_results to indicate context was created successfully\n",
    "    step2_results[\"context_created\"] = True\n",
    "    step2_results[\"context_type\"] = type(context).__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd991818-3748-4d55-90ad-6c83096839e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nCONTEXT API DISCOVERY\n--------------------------------------------------\n\nDATASOURCE METHODS (9):\n  - add_datasource\n  - add_or_update_datasource\n  - data_sources\n  - datasource_store\n  - delete_datasource\n  - fluent_datasources\n  - get_datasource\n  - list_datasources\n  - update_datasource\n\nEXPECTATION METHODS (3):\n  - expectations_store\n  - expectations_store_name\n  - suites\n\nVALIDATION METHODS (9):\n  - get_validation_result\n  - get_validator\n  - get_validator_using_batch_list\n  - store_validation_result_metrics\n  - validation_definition_store\n  - validation_definitions\n  - validation_results_store\n  - validation_results_store_name\n  - view_validation_result\n\nDATA_DOCS METHODS (8):\n  - add_data_docs_site\n  - build_data_docs\n  - clean_data_docs\n  - delete_data_docs_site\n  - get_docs_sites_urls\n  - list_data_docs_sites\n  - open_data_docs\n  - update_data_docs_site\n\nSTORE METHODS (7):\n  - add_store\n  - checkpoint_store\n  - checkpoint_store_name\n  - delete_store\n  - list_active_stores\n  - list_stores\n  - stores\n\nCHECKPOINT METHODS (2):\n  - checkpoints\n  - prepare_checkpoint_run\n\nOTHER METHODS (44):\n  - BASE_DIRECTORIES\n  - DOLLAR_SIGN_ESCAPE_STRING\n  - GITIGNORE\n  - GLOBAL_CONFIG_PATHS\n  - GX_CONFIG_VARIABLES\n  - GX_DIR\n  - GX_EDIT_NOTEBOOK_DIR\n  - GX_UNCOMMITTED_DIR\n  - GX_YML\n  - UNCOMMITTED_DIRECTORIES\n  - all_uncommitted_directories_exist\n  - build_batch_kwargs\n  - config\n  - config_provider\n  - config_variables\n  - config_variables_yml_exist\n  - data_context_id\n  - does_config_exist_on_disk\n  - enable_analytics\n  - escape_all_config_variables\n  - find_context_root_dir\n  - fluent_config\n  - get_available_data_asset_names\n  - get_config\n  - get_config_with_variables_substituted\n  - get_ge_config_version\n  - get_last_batch\n  - get_or_create_data_context_config\n  - get_site_names\n  - instance_id\n  - is_project_initialized\n  - is_project_scaffolded\n  - mode\n  - plugins_directory\n  - progress_bars\n  - project_config_with_variables_substituted\n  - root_directory\n  - runtime_environment\n  - save_config_variable\n  - set_config\n  - set_ge_config_version\n  - set_user_agent_str\n  - update_project_config\n  - variables\n\nTESTING KEY METHODS\n------------------------------\nlist_datasources - Available\nadd_datasource - Available\nget_datasource - Available\n"
     ]
    }
   ],
   "source": [
    "# # total available Methods by GX\n",
    "if context is not None:\n",
    "    print(\"\\nCONTEXT API DISCOVERY\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    all_methods = [method for method in dir(context) if not method.startswith('_')]\n",
    "    step2_results[\"context_methods\"] = all_methods\n",
    "\n",
    "    method_categories = {\n",
    "        \"datasource\": [],\n",
    "        \"expectation\": [], \n",
    "        \"validation\": [],\n",
    "        \"data_docs\": [],\n",
    "        \"store\": [],\n",
    "        \"checkpoint\": [],\n",
    "        \"other\": []\n",
    "    }\n",
    "\n",
    "    for method in all_methods:\n",
    "        categorized = False\n",
    "        if any(keyword in method.lower() for keyword in ['datasource', 'data_source']):\n",
    "            method_categories[\"datasource\"].append(method)\n",
    "            categorized = True\n",
    "        elif any(keyword in method.lower() for keyword in ['expectation', 'suite', 'expect']):\n",
    "            method_categories[\"expectation\"].append(method)\n",
    "            categorized = True\n",
    "        elif any(keyword in method.lower() for keyword in ['validation', 'validate', 'validator']):\n",
    "            method_categories[\"validation\"].append(method)\n",
    "            categorized = True\n",
    "        elif any(keyword in method.lower() for keyword in ['docs', 'documentation']):\n",
    "            method_categories[\"data_docs\"].append(method)\n",
    "            categorized = True\n",
    "        elif any(keyword in method.lower() for keyword in ['store']):\n",
    "            method_categories[\"store\"].append(method)\n",
    "            categorized = True\n",
    "        elif any(keyword in method.lower() for keyword in ['checkpoint']):\n",
    "            method_categories[\"checkpoint\"].append(method)\n",
    "            categorized = True\n",
    "\n",
    "        if not categorized:\n",
    "            method_categories[\"other\"].append(method)\n",
    "\n",
    "    step2_results[\"method_categories\"] = method_categories\n",
    "\n",
    "    for category, methods in method_categories.items():\n",
    "        if methods:\n",
    "            print(f\"\\n{category.upper()} METHODS ({len(methods)}):\")\n",
    "            for method in sorted(methods)[:100]:\n",
    "                print(f\"  - {method}\")\n",
    "            if len(methods) > 100:\n",
    "                print(f\"  ... and {len(methods) - 100} more\")\n",
    "\n",
    "    print(\"\\nTESTING KEY METHODS\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    key_methods = ['list_datasources', 'add_datasource', 'get_datasource']\n",
    "    method_test_results = {}\n",
    "\n",
    "    for method_name in key_methods:\n",
    "        if hasattr(context, method_name):\n",
    "            try:\n",
    "                method = getattr(context, method_name)\n",
    "                method_test_results[method_name] = \"available\"\n",
    "                print(f\"{method_name} - Available\")\n",
    "            except Exception as e:\n",
    "                method_test_results[method_name] = f\"error: {e}\"\n",
    "                print(f\"{method_name} - Available but error: {e}\")\n",
    "        else:\n",
    "            method_test_results[method_name] = \"missing\"\n",
    "            print(f\"{method_name} - Missing\")\n",
    "\n",
    "    step2_results[\"key_method_tests\"] = method_test_results\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo context available for API discovery\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94beca7f-bb7f-4679-b2a0-3b5d00e492b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONTEXT CONFIGURATION & READINESS CHECK\n",
    "# =============================================================================\n",
    "\n",
    "if context is not None:\n",
    "    print(f\"CONTEXT CONFIGURATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get context configuration details\n",
    "    try:\n",
    "        # Try to get root directory\n",
    "        if hasattr(context, 'root_directory'):\n",
    "            step2_results[\"context_config\"][\"root_directory\"] = getattr(context, 'root_directory', 'Unknown')\n",
    "            print(f\"Root directory: {step2_results['context_config']['root_directory']}\")\n",
    "        \n",
    "        # Try to list datasources\n",
    "        try:\n",
    "            datasources = context.list_datasources()\n",
    "            step2_results[\"context_config\"][\"datasources\"] = len(datasources)\n",
    "            print(f\"Available datasources: {len(datasources)}\")\n",
    "            \n",
    "            for ds in datasources[:3]:  # Show first 3 datasources\n",
    "                ds_name = ds.get('name', 'unnamed') if isinstance(ds, dict) else str(ds)\n",
    "                print(f\"  - {ds_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Could not list datasources: {e}\")\n",
    "            step2_results[\"context_config\"][\"datasources\"] = \"error\"\n",
    "        \n",
    "        # Check if context has store configurations\n",
    "        store_attributes = ['expectations_store', 'validations_store', 'checkpoint_store']\n",
    "        for store_attr in store_attributes:\n",
    "            if hasattr(context, store_attr):\n",
    "                step2_results[\"context_config\"][store_attr] = \"available\"\n",
    "                print(f\"{store_attr}: Available\")\n",
    "            else:\n",
    "                step2_results[\"context_config\"][store_attr] = \"missing\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting context configuration: {e}\")\n",
    "        step2_results[\"context_config\"][\"error\"] = str(e)\n",
    "\n",
    "# Final readiness assessment\n",
    "ready_for_step3 = (\n",
    "    step2_results[\"context_created\"] and\n",
    "    step2_results.get(\"key_method_tests\", {}).get(\"list_datasources\") in [\"available\", \"error\"]  # Allow error as some methods might fail without data\n",
    ")\n",
    "\n",
    "step2_results[\"ready_for_step3\"] = ready_for_step3\n",
    "step2_results[\"status\"] = \"success\" if ready_for_step3 else \"error\"\n",
    "\n",
    "if ready_for_step3:\n",
    "    print(f\"\\n\uD83C\uDF89 STEP 2 COMPLETED SUCCESSFULLY\")\n",
    "    print(f\"GX Context ready for Step 3: SQL Connection & Data Loading\")\n",
    "    print(f\"Context type: {step2_results['context_type']}\")\n",
    "    print(f\"Available methods: {len(step2_results['context_methods'])}\")\n",
    "else:\n",
    "    print(f\"STEP 2 FAILED\")\n",
    "    print(f\"Issues:\")\n",
    "    if step2_results.get(\"error_message\"):\n",
    "        print(f\"   â€¢ {step2_results['error_message']}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Store context in a way that can be accessed by subsequent steps\n",
    "# Note: In production, you might want to use dbutils.fs to store context configuration\n",
    "step2_results[\"context_ready\"] = context is not None\n",
    "\n",
    "# Return results for orchestrator\n",
    "dbutils.notebook.exit(json.dumps(step2_results))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "step2_gx_context_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}