{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a34c619d-83b3-40f8-9c81-7f16bec83412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 4.5: Data Quality Rules Definition Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e9e549d-3000-46ac-97e3-9b8bbdb012fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python environment: /local_disk0/.ephemeral_nfs/envs/pythonEnv-08a0eb54-6f7c-4845-bd4d-6607a9f6a80d/bin/python\nSUCCESS: Great Expectations 1.5.7 is available!\nModule location: /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/great_expectations/__init__.py\nReady to proceed with Step 4 validation setup\nDatabricks environment confirmed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "print(f\"Python environment: {sys.executable}\")\n",
    "\n",
    "# Step 1: Try to import Great Expectations\n",
    "try:\n",
    "    import great_expectations as gx\n",
    "    print(f\"SUCCESS: Great Expectations {gx.__version__} is available!\")\n",
    "    print(f\"Module location: {gx.__file__}\")\n",
    "    print(f\"Ready to proceed with Step 4 validation setup\")\n",
    "    \n",
    "    # Verify Databricks environment\n",
    "    try:\n",
    "        dbutils.fs.ls('/')\n",
    "        print(\"Databricks environment confirmed\")\n",
    "    except NameError:\n",
    "        print(\"Warning: dbutils not available\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Great Expectations not found - Auto-installing now...\")\n",
    "    \n",
    "    # Check if we're in Databricks\n",
    "    try:\n",
    "        dbutils.fs.ls('/')\n",
    "        print(\"Databricks environment confirmed\")\n",
    "        \n",
    "        print(\"AUTO-INSTALLING WITH DATABRICKS %pip...\")\n",
    "        print(\"Installing: great-expectations[sql,azure,databricks]...\")\n",
    "        \n",
    "        # Auto-install using %pip magic\n",
    "        get_ipython().run_line_magic('pip', 'install great-expectations[sql,azure,databricks]')\n",
    "        \n",
    "        print(\"Installation completed!\")\n",
    "        print(\"AUTO-RESTARTING PYTHON ENVIRONMENT...\")\n",
    "        \n",
    "  \n",
    "        dbutils.library.restartPython()\n",
    "        \n",
    "    except NameError:\n",
    "        print(\"Not in Databricks - manual installation required\")\n",
    "        print(\"MANUAL INSTALLATION REQUIRED:\")\n",
    "        print(\"pip install great-expectations[sql]\")\n",
    "        raise ImportError(\"Manual installation required - not in Databricks environment\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Auto-installation failed: {e}\")\n",
    "        print(\"FALLBACK - RUN THESE COMMANDS MANUALLY:\")\n",
    "        print(\"1. %pip install great-expectations[sql,azure,databricks]\")\n",
    "        print(\"2. dbutils.library.restartPython()\")\n",
    "        print(\"3. Re-run this cell\")\n",
    "        raise ImportError(\"Auto-installation failed - use manual commands above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ce3769-8750-4c97-b78d-3a764237c693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4.5 - DBFS CONTEXT SETUP\n--------------------------------------------------\nCurrent Great Expectations 1.5.7 \nDBFS context path: /dbfs/FileStore/great_expectations\nContext type: FileDataContext\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DBFS CONTEXT SETUP (Connect to existing GX context from Step 2)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"STEP 4.5 - DBFS CONTEXT SETUP\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "import os\n",
    "\n",
    "# After restart, re-import Great Expectations\n",
    "try:\n",
    "    import great_expectations as gx\n",
    "    print(f\"Current Great Expectations {gx.__version__} \")\n",
    "    \n",
    "\n",
    "    is_databricks = False\n",
    "    try:\n",
    "        dbutils.fs.ls('/')\n",
    "        is_databricks = True\n",
    "    \n",
    "    except NameError:\n",
    "\n",
    "        is_databricks = False\n",
    "    \n",
    "    \n",
    "    if is_databricks:\n",
    "        dbfs_gx_path = \"/dbfs/FileStore/great_expectations\"\n",
    "        print(f\"DBFS context path: {dbfs_gx_path}\")\n",
    "        \n",
    "  \n",
    "        if os.path.exists(dbfs_gx_path):\n",
    "            context = gx.get_context(project_root_dir=dbfs_gx_path)\n",
    "         \n",
    "            print(f\"Context type: {type(context).__name__}\")\n",
    "        else:\n",
    "          \n",
    "            os.makedirs(dbfs_gx_path, exist_ok=True)\n",
    "            context = gx.get_context(project_root_dir=dbfs_gx_path)\n",
    "          \n",
    "    else:\n",
    "        \n",
    "        local_gx_path = os.path.join(os.getcwd(), \"great_expectations\")\n",
    "        context = gx.get_context(project_root_dir=local_gx_path)\n",
    "       \n",
    "    \n",
    "\n",
    "    \n",
    "except ImportError as e:\n",
    "\n",
    "    raise ImportError(\"Great Expectations not available - run installation cell above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2eb9475-360f-4e37-acac-b2d0b3306996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Rules initialization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "\n",
    "step4_5_results = {\n",
    "    \"status\": \"running\",\n",
    "    \"rules_defined\": 0,\n",
    "    \"rule_categories\": {},\n",
    "    \"custom_rules\": [],\n",
    "    \"table_specific_rules\": {},\n",
    "    \"error_message\": None\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3adc8b9a-544b-43a0-bf82-1231e7e1ce33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core rules library functions defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CORE RULES-SPECIFIC FUNCTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "\n",
    "step4_5_results = {\n",
    "    \"status\": \"running\",\n",
    "    \"rules_defined\": 0,\n",
    "    \"rule_categories\": {},\n",
    "    \"custom_rules\": [],\n",
    "    \"table_specific_rules\": {},\n",
    "    \"error_message\": None\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def get_core_data_quality_rules() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Core data quality rules that apply to most datasets\n",
    "    These are fundamental checks that should be performed on any table\n",
    "    \"\"\"\n",
    "    \n",
    "    core_rules = [\n",
    "        # Table-level rules\n",
    "        {\n",
    "            \"rule_id\": \"TBL_001\",\n",
    "            \"category\": \"Critical\",\n",
    "            \"expectation_type\": \"expect_table_row_count_to_be_between\",\n",
    "            \"kwargs\": {\"min_value\": 1, \"max_value\": 1000000},\n",
    "            \"meta\": {\n",
    "                \"description\": \"Table should have reasonable number of rows\",\n",
    "                \"business_impact\": \"Empty tables or extremely large tables may indicate data pipeline issues\",\n",
    "                \"remediation\": \"Check data pipeline and source systems\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"rule_id\": \"TBL_002\", \n",
    "            \"category\": \"Important\",\n",
    "            \"expectation_type\": \"expect_table_column_count_to_be_between\",\n",
    "            \"kwargs\": {\"min_value\": 1, \"max_value\": 100},\n",
    "            \"meta\": {\n",
    "                \"description\": \"Table should have reasonable number of columns\",\n",
    "                \"business_impact\": \"Schema changes may affect downstream systems\",\n",
    "                \"remediation\": \"Verify table schema matches expectations\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return core_rules\n",
    "\n",
    "def get_hierarchy_id_rules() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Rules specific to HIERARCHY_ID column\n",
    "    These are critical for maintaining data integrity in hierarchical structures\n",
    "    \"\"\"\n",
    "    \n",
    "    hierarchy_rules = [\n",
    "        {\n",
    "            \"rule_id\": \"HIE_001\",\n",
    "            \"category\": \"Critical\",\n",
    "            \"expectation_type\": \"expect_column_to_exist\",\n",
    "            \"kwargs\": {\"column\": \"HIERARCHY_ID\"},\n",
    "            \"meta\": {\n",
    "                \"description\": \"HIERARCHY_ID column must exist\",\n",
    "                \"business_impact\": \"Missing hierarchy column breaks organizational structure\",\n",
    "                \"remediation\": \"Verify table schema and data source\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"rule_id\": \"HIE_002\",\n",
    "            \"category\": \"Critical\", \n",
    "            \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
    "            \"kwargs\": {\"column\": \"HIERARCHY_ID\"},\n",
    "            \"meta\": {\n",
    "                \"description\": \"HIERARCHY_ID should not be null\",\n",
    "                \"business_impact\": \"Null hierarchy IDs break organizational reporting\",\n",
    "                \"remediation\": \"Investigate source data quality and implement null checks\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"rule_id\": \"HIE_003\",\n",
    "            \"category\": \"Important\",\n",
    "            \"expectation_type\": \"expect_column_values_to_be_unique\",\n",
    "            \"kwargs\": {\"column\": \"HIERARCHY_ID\"},\n",
    "            \"meta\": {\n",
    "                \"description\": \"HIERARCHY_ID should be unique\",\n",
    "                \"business_impact\": \"Duplicate hierarchy IDs cause reporting inconsistencies\",\n",
    "                \"remediation\": \"Implement unique constraints and deduplication logic\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"rule_id\": \"HIE_004\",\n",
    "            \"category\": \"Important\",\n",
    "            \"expectation_type\": \"expect_column_values_to_match_regex\",\n",
    "            \"kwargs\": {\"column\": \"HIERARCHY_ID\", \"regex\": r\"^[A-Z0-9_-]+$\"},\n",
    "            \"meta\": {\n",
    "                \"description\": \"HIERARCHY_ID should follow standard format\",\n",
    "                \"business_impact\": \"Non-standard formats may cause integration issues\",\n",
    "                \"remediation\": \"Standardize hierarchy ID format across systems\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return hierarchy_rules\n",
    "\n",
    "def get_date_column_rules() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Rules for date/timestamp columns like RECORD_CREATE_DATE\n",
    "    These ensure audit trail integrity\n",
    "    \"\"\"\n",
    "    \n",
    "    date_rules = [\n",
    "        {\n",
    "            \"rule_id\": \"DTE_001\",\n",
    "            \"category\": \"Critical\",\n",
    "            \"expectation_type\": \"expect_column_to_exist\", \n",
    "            \"kwargs\": {\"column\": \"RECORD_CREATE_DATE\"},\n",
    "            \"meta\": {\n",
    "                \"description\": \"RECORD_CREATE_DATE column must exist\",\n",
    "                \"business_impact\": \"Missing audit dates affect compliance and traceability\",\n",
    "                \"remediation\": \"Ensure audit columns are included in data pipeline\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"rule_id\": \"DTE_002\",\n",
    "            \"category\": \"Critical\",\n",
    "            \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
    "            \"kwargs\": {\"column\": \"RECORD_CREATE_DATE\"},\n",
    "            \"meta\": {\n",
    "                \"description\": \"RECORD_CREATE_DATE should not be null\", \n",
    "                \"business_impact\": \"Missing audit timestamps affect regulatory compliance\",\n",
    "                \"remediation\": \"Implement default timestamp logic in data pipeline\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"rule_id\": \"DTE_003\",\n",
    "            \"category\": \"Important\",\n",
    "            \"expectation_type\": \"expect_column_values_to_be_between\",\n",
    "            \"kwargs\": {\n",
    "                \"column\": \"RECORD_CREATE_DATE\",\n",
    "                \"min_value\": \"2020-01-01\",\n",
    "                \"max_value\": \"2030-12-31\"\n",
    "            },\n",
    "            \"meta\": {\n",
    "                \"description\": \"RECORD_CREATE_DATE should be within reasonable range\",\n",
    "                \"business_impact\": \"Invalid dates may indicate data corruption\",\n",
    "                \"remediation\": \"Validate date ranges in source systems\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return date_rules\n",
    "\n",
    "print(\"Core rules library functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51110a84-8515-47c3-9ff8-963f2ffe6406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business-specific rules functions defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BUSINESS-SPECIFIC RULES (CUSTOMIZABLE BY TEAMS)\n",
    "# =============================================================================\n",
    "\n",
    "def get_business_specific_rules() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Business-specific rules that teams can customize\n",
    "    Add your domain-specific validation rules here\n",
    "    \"\"\"\n",
    "    \n",
    "    business_rules = [\n",
    "        {\n",
    "            \"rule_id\": \"BUS_001\",\n",
    "            \"category\": \"Important\",\n",
    "            \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
    "            \"kwargs\": {\"column\": \"STATUS\", \"mostly\": 0.95},\n",
    "            \"meta\": {\n",
    "                \"description\": \"STATUS column should be mostly non-null\",\n",
    "                \"business_impact\": \"Missing status affects operational reporting\",\n",
    "                \"remediation\": \"Implement default status values\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"rule_id\": \"BUS_002\", \n",
    "            \"category\": \"Optional\",\n",
    "            \"expectation_type\": \"expect_column_values_to_be_in_set\",\n",
    "            \"kwargs\": {\"column\": \"PRIORITY\", \"value_set\": [\"HIGH\", \"MEDIUM\", \"LOW\"]},\n",
    "            \"meta\": {\n",
    "                \"description\": \"PRIORITY should be from valid set\",\n",
    "                \"business_impact\": \"Invalid priority values affect workflow routing\",\n",
    "                \"remediation\": \"Standardize priority value lists\"\n",
    "            }\n",
    "        }\n",
    "        # ADD YOUR CUSTOM BUSINESS RULES HERE\n",
    "        # Example:\n",
    "        # {\n",
    "        #     \"rule_id\": \"BUS_003\",\n",
    "        #     \"category\": \"Critical\",\n",
    "        #     \"expectation_type\": \"expect_column_values_to_be_between\",\n",
    "        #     \"kwargs\": {\"column\": \"COST\", \"min_value\": 0, \"max_value\": 1000000},\n",
    "        #     \"meta\": {\n",
    "        #         \"description\": \"Cost values should be within business limits\",\n",
    "        #         \"business_impact\": \"Invalid costs affect financial reporting\",\n",
    "        #         \"remediation\": \"Review cost calculation logic\"\n",
    "        #     }\n",
    "        # }\n",
    "    ]\n",
    "    \n",
    "    return business_rules\n",
    "\n",
    "def get_data_type_rules() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Data type and format validation rules\n",
    "    These ensure data types and formats are consistent\n",
    "    \"\"\"\n",
    "    \n",
    "    datatype_rules = [\n",
    "        {\n",
    "            \"rule_id\": \"DTP_001\",\n",
    "            \"category\": \"Important\",\n",
    "            \"expectation_type\": \"expect_column_values_to_match_regex\",\n",
    "            \"kwargs\": {\"column\": \"EMAIL\", \"regex\": r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"},\n",
    "            \"meta\": {\n",
    "                \"description\": \"EMAIL should follow valid email format\",\n",
    "                \"business_impact\": \"Invalid emails affect communication and notifications\",\n",
    "                \"remediation\": \"Implement email validation in input forms\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"rule_id\": \"DTP_002\",\n",
    "            \"category\": \"Important\", \n",
    "            \"expectation_type\": \"expect_column_values_to_match_regex\",\n",
    "            \"kwargs\": {\"column\": \"PHONE\", \"regex\": r\"^\\+?[\\d\\s\\-\\(\\)]+$\"},\n",
    "            \"meta\": {\n",
    "                \"description\": \"PHONE should follow valid phone format\",\n",
    "                \"business_impact\": \"Invalid phone numbers affect customer contact\",\n",
    "                \"remediation\": \"Standardize phone number format\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return datatype_rules\n",
    "\n",
    "print(\"Business-specific rules functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42855b8-c0dd-4ce5-8c54-4256a86be884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table-specific rules configuration defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TABLE-SPECIFIC RULES CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "def get_table_specific_rules() -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Rules specific to particular tables\n",
    "    Teams can add rules for specific tables here\n",
    "    \"\"\"\n",
    "    \n",
    "    table_rules = {\n",
    "        \"DQ_LOGIC\": [\n",
    "            {\n",
    "                \"rule_id\": \"DQL_001\",\n",
    "                \"category\": \"Critical\",\n",
    "                \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
    "                \"kwargs\": {\"column\": \"LOGIC_TYPE\", \"mostly\": 0.98},\n",
    "                \"meta\": {\n",
    "                    \"description\": \"LOGIC_TYPE should be almost always populated for DQ_LOGIC table\",\n",
    "                    \"business_impact\": \"Missing logic types affect data quality categorization\",\n",
    "                    \"remediation\": \"Review data quality logic classification\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"rule_id\": \"DQL_002\", \n",
    "                \"category\": \"Important\",\n",
    "                \"expectation_type\": \"expect_column_distinct_values_to_be_in_set\",\n",
    "                \"kwargs\": {\"column\": \"ACTIVE_FLAG\", \"value_set\": [\"Y\", \"N\", \"1\", \"0\", \"TRUE\", \"FALSE\"]},\n",
    "                \"meta\": {\n",
    "                    \"description\": \"ACTIVE_FLAG should use standard boolean values\",\n",
    "                    \"business_impact\": \"Non-standard flags affect filtering logic\",\n",
    "                    \"remediation\": \"Standardize boolean value representation\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        \"USER_PROFILES\": [\n",
    "            {\n",
    "                \"rule_id\": \"USR_001\",\n",
    "                \"category\": \"Critical\",\n",
    "                \"expectation_type\": \"expect_column_values_to_be_unique\",\n",
    "                \"kwargs\": {\"column\": \"USER_ID\"},\n",
    "                \"meta\": {\n",
    "                    \"description\": \"USER_ID must be unique in USER_PROFILES\",\n",
    "                    \"business_impact\": \"Duplicate user IDs cause authentication issues\",\n",
    "                    \"remediation\": \"Implement unique constraints on user ID\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # ADD YOUR TABLE-SPECIFIC RULES HERE\n",
    "        # \"YOUR_TABLE_NAME\": [\n",
    "        #     {\n",
    "        #         \"rule_id\": \"YTN_001\",\n",
    "        #         \"category\": \"Critical\",\n",
    "        #         \"expectation_type\": \"your_expectation_type\",\n",
    "        #         \"kwargs\": {\"column\": \"your_column\"},\n",
    "        #         \"meta\": {\n",
    "        #             \"description\": \"Your rule description\",\n",
    "        #             \"business_impact\": \"Impact on business\",\n",
    "        #             \"remediation\": \"How to fix issues\"\n",
    "        #         }\n",
    "        #     }\n",
    "        # ]\n",
    "    }\n",
    "    \n",
    "    return table_rules\n",
    "\n",
    "print(\"Table-specific rules configuration defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45eeb4d6-cb05-44e1-9446-79a037304088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rules compilation and validation functions defined\n"
     ]
    }
   ],
   "source": [
    "# ALL RULES CHECK\n",
    "\n",
    "def compile_rules_for_table(table_name: str, available_columns: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Compile all applicable rules for a specific table\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table to validate\n",
    "        available_columns: List of columns available in the table\n",
    "        \n",
    "    Returns:\n",
    "        List of applicable rules for the table\n",
    "    \"\"\"\n",
    "    \n",
    "    all_rules = []\n",
    "    \n",
    "    # 1. Add core rules (apply to all tables)\n",
    "    all_rules.extend(get_core_data_quality_rules())\n",
    "    \n",
    "    # 2. Add hierarchy rules if HIERARCHY_ID column exists\n",
    "    if \"HIERARCHY_ID\" in available_columns:\n",
    "        all_rules.extend(get_hierarchy_id_rules())\n",
    "    \n",
    "    # 3. Add date rules if RECORD_CREATE_DATE column exists  \n",
    "    if \"RECORD_CREATE_DATE\" in available_columns:\n",
    "        all_rules.extend(get_date_column_rules())\n",
    "    \n",
    "    # 4. Add business rules (filter by available columns)\n",
    "    business_rules = get_business_specific_rules()\n",
    "    for rule in business_rules:\n",
    "        if \"column\" in rule[\"kwargs\"]:\n",
    "            if rule[\"kwargs\"][\"column\"] in available_columns:\n",
    "                all_rules.append(rule)\n",
    "        else:\n",
    "            # Table-level rules without specific columns\n",
    "            all_rules.append(rule)\n",
    "    \n",
    "    # 5. Add data type rules (filter by available columns)\n",
    "    datatype_rules = get_data_type_rules()\n",
    "    for rule in datatype_rules:\n",
    "        if \"column\" in rule[\"kwargs\"]:\n",
    "            if rule[\"kwargs\"][\"column\"] in available_columns:\n",
    "                all_rules.append(rule)\n",
    "    \n",
    "    # 6. Add table-specific rules\n",
    "    table_rules = get_table_specific_rules()\n",
    "    if table_name in table_rules:\n",
    "        for rule in table_rules[table_name]:\n",
    "            if \"column\" in rule[\"kwargs\"]:\n",
    "                if rule[\"kwargs\"][\"column\"] in available_columns:\n",
    "                    all_rules.append(rule)\n",
    "            else:\n",
    "                all_rules.append(rule)\n",
    "    \n",
    "    return all_rules\n",
    "\n",
    "def validate_rules(rules: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate that rules are properly formatted\n",
    "    \n",
    "    Args:\n",
    "        rules: List of rules to validate\n",
    "        \n",
    "    Returns:\n",
    "        Validation results\n",
    "    \"\"\"\n",
    "    \n",
    "    validation_results = {\n",
    "        \"valid_rules\": 0,\n",
    "        \"invalid_rules\": 0,\n",
    "        \"errors\": []\n",
    "    }\n",
    "    \n",
    "    required_fields = [\"rule_id\", \"category\", \"expectation_type\", \"kwargs\", \"meta\"]\n",
    "    \n",
    "    for i, rule in enumerate(rules):\n",
    "        try:\n",
    "            # Check required fields\n",
    "            for field in required_fields:\n",
    "                if field not in rule:\n",
    "                    raise ValueError(f\"Missing required field: {field}\")\n",
    "            \n",
    "            # Check meta description\n",
    "            if \"description\" not in rule[\"meta\"]:\n",
    "                raise ValueError(\"Missing description in meta\")\n",
    "            \n",
    "            # Check category is valid\n",
    "            if rule[\"category\"] not in [\"Critical\", \"Important\", \"Optional\"]:\n",
    "                raise ValueError(f\"Invalid category: {rule['category']}\")\n",
    "            \n",
    "            validation_results[\"valid_rules\"] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            validation_results[\"invalid_rules\"] += 1\n",
    "            validation_results[\"errors\"].append(f\"Rule {i+1}: {str(e)}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "print(\"Rules compilation and validation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7cd97c9-eff5-42c0-9ca5-447fdb246dfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPILING RULES FOR DEMONSTRATION\n--------------------------------------------------\nRules compilation completed\n   Total rules defined: 12\n   Critical rules: 6\n   Important rules: 6\n   Optional rules: 0\n   Valid rules: 12\n   Invalid rules: 0\nCOMPILED RULES SUMMARY:\n--------------------------------------------------\n  1. [Critical] Table should have reasonable number of rows\n  2. [Important] Table should have reasonable number of columns\n  3. [Critical] HIERARCHY_ID column must exist\n  4. [Critical] HIERARCHY_ID should not be null\n  5. [Important] HIERARCHY_ID should be unique\n  6. [Important] HIERARCHY_ID should follow standard format\n  7. [Critical] RECORD_CREATE_DATE column must exist\n  8. [Critical] RECORD_CREATE_DATE should not be null\n  9. [Important] RECORD_CREATE_DATE should be within reasonable range\n  10. [Important] STATUS column should be mostly non-null\n  11. [Critical] LOGIC_TYPE should be almost always populated for DQ_LOGIC table\n  12. [Important] ACTIVE_FLAG should use standard boolean values\nSTEP 4.5 COMPLETED\nRules library ready for Step 5\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# FINAL COMPILED RULES RUN\n",
    "\n",
    "try:\n",
    "    print(f\"COMPILING RULES FOR DEMONSTRATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Simulate available columns for specific table\n",
    "    sample_columns = [\"HIERARCHY_ID\", \"RECORD_CREATE_DATE\", \"STATUS\", \"LOGIC_TYPE\", \"ACTIVE_FLAG\"]\n",
    "    \n",
    "    # Compile rules for specific table\n",
    "    compiled_rules = compile_rules_for_table(\"dq_error_result\", sample_columns)\n",
    "    \n",
    "    # Validate rules\n",
    "    validation_results = validate_rules(compiled_rules)\n",
    "    \n",
    "    # Categorize rules\n",
    "    rule_categories = {\n",
    "        \"Critical\": [r for r in compiled_rules if r[\"category\"] == \"Critical\"],\n",
    "        \"Important\": [r for r in compiled_rules if r[\"category\"] == \"Important\"], \n",
    "        \"Optional\": [r for r in compiled_rules if r[\"category\"] == \"Optional\"]\n",
    "    }\n",
    "    \n",
    "    # Update results\n",
    "    step4_5_results.update({\n",
    "        \"rules_defined\": len(compiled_rules),\n",
    "        \"rule_categories\": {\n",
    "            \"Critical\": len(rule_categories[\"Critical\"]),\n",
    "            \"Important\": len(rule_categories[\"Important\"]),\n",
    "            \"Optional\": len(rule_categories[\"Optional\"])\n",
    "        },\n",
    "        \"custom_rules\": compiled_rules,\n",
    "        \"table_specific_rules\": get_table_specific_rules(),\n",
    "        \"validation_results\": validation_results,\n",
    "        \"status\": \"success\"\n",
    "    })\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"Rules compilation completed\")\n",
    "    print(f\"   Total rules defined: {len(compiled_rules)}\")\n",
    "    print(f\"   Critical rules: {len(rule_categories['Critical'])}\")\n",
    "    print(f\"   Important rules: {len(rule_categories['Important'])}\")\n",
    "    print(f\"   Optional rules: {len(rule_categories['Optional'])}\")\n",
    "    print(f\"   Valid rules: {validation_results['valid_rules']}\")\n",
    "    print(f\"   Invalid rules: {validation_results['invalid_rules']}\")\n",
    "    \n",
    "    # Display rule details\n",
    "    print(f\"COMPILED RULES SUMMARY:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, rule in enumerate(compiled_rules, 1):\n",
    "        print(f\"  {i}. [{rule['category']}] {rule['meta']['description']}\")\n",
    "    \n",
    "    if validation_results[\"errors\"]:\n",
    "        print(f\"VALIDATION ERRORS:\")\n",
    "        for error in validation_results[\"errors\"]:\n",
    "            print(f\"   • {error}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Rules compilation failed: {e}\")\n",
    "    step4_5_results[\"error_message\"] = f\"Rules compilation failed: {e}\"\n",
    "    step4_5_results[\"status\"] = \"error\"\n",
    "\n",
    "print(f\"STEP 4.5 COMPLETED\")\n",
    "print(f\"Rules library ready for Step 5\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "155f91a5-9911-4e0b-b529-4d56bcffc686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{\"status\": \"success\", \"rules_defined\": 12, \"rule_categories\": {\"Critical\": 6, \"Important\": 6, \"Optional\": 0}, \"custom_rules\": [{\"rule_id\": \"TBL_001\", \"category\": \"Critical\", \"expectation_type\": \"expect_table_row_count_to_be_between\", \"kwargs\": {\"min_value\": 1, \"max_value\": 1000000}, \"meta\": {\"description\": \"Table should have reasonable number of rows\", \"business_impact\": \"Empty tables or extremely large tables may indicate data pipeline issues\", \"remediation\": \"Check data pipeline and source systems\"}}, {\"rule_id\": \"TBL_002\", \"category\": \"Important\", \"expectation_type\": \"expect_table_column_count_to_be_between\", \"kwargs\": {\"min_value\": 1, \"max_value\": 100}, \"meta\": {\"description\": \"Table should have reasonable number of columns\", \"business_impact\": \"Schema changes may affect downstream systems\", \"remediation\": \"Verify table schema matches expectations\"}}, {\"rule_id\": \"HIE_001\", \"category\": \"Critical\", \"expectation_type\": \"expect_column_to_exist\", \"kwargs\": {\"column\": \"HIERARCHY_ID\"}, \"meta\": {\"description\": \"HIERARCHY_ID column must exist\", \"business_impact\": \"Missing hierarchy column breaks organizational structure\", \"remediation\": \"Verify table schema and data source\"}}, {\"rule_id\": \"HIE_002\", \"category\": \"Critical\", \"expectation_type\": \"expect_column_values_to_not_be_null\", \"kwargs\": {\"column\": \"HIERARCHY_ID\"}, \"meta\": {\"description\": \"HIERARCHY_ID should not be null\", \"business_impact\": \"Null hierarchy IDs break organizational reporting\", \"remediation\": \"Investigate source data quality and implement null checks\"}}, {\"rule_id\": \"HIE_003\", \"category\": \"Important\", \"expectation_type\": \"expect_column_values_to_be_unique\", \"kwargs\": {\"column\": \"HIERARCHY_ID\"}, \"meta\": {\"description\": \"HIERARCHY_ID should be unique\", \"business_impact\": \"Duplicate hierarchy IDs cause reporting inconsistencies\", \"remediation\": \"Implement unique constraints and deduplication logic\"}}, {\"rule_id\": \"HIE_004\", \"category\": \"Important\", \"expectation_type\": \"expect_column_values_to_match_regex\", \"kwargs\": {\"column\": \"HIERARCHY_ID\", \"regex\": \"^[A-Z0-9_-]+$\"}, \"meta\": {\"description\": \"HIERARCHY_ID should follow standard format\", \"business_impact\": \"Non-standard formats may cause integration issues\", \"remediation\": \"Standardize hierarchy ID format across systems\"}}, {\"rule_id\": \"DTE_001\", \"category\": \"Critical\", \"expectation_type\": \"expect_column_to_exist\", \"kwargs\": {\"column\": \"RECORD_CREATE_DATE\"}, \"meta\": {\"description\": \"RECORD_CREATE_DATE column must exist\", \"business_impact\": \"Missing audit dates affect compliance and traceability\", \"remediation\": \"Ensure audit columns are included in data pipeline\"}}, {\"rule_id\": \"DTE_002\", \"category\": \"Critical\", \"expectation_type\": \"expect_column_values_to_not_be_null\", \"kwargs\": {\"column\": \"RECORD_CREATE_DATE\"}, \"meta\": {\"description\": \"RECORD_CREATE_DATE should not be null\", \"business_impact\": \"Missing audit timestamps affect regulatory compliance\", \"remediation\": \"Implement default timestamp logic in data pipeline\"}}, {\"rule_id\": \"DTE_003\", \"category\": \"Important\", \"expectation_type\": \"expect_column_values_to_be_between\", \"kwargs\": {\"column\": \"RECORD_CREATE_DATE\", \"min_value\": \"2020-01-01\", \"max_value\": \"2030-12-31\"}, \"meta\": {\"description\": \"RECORD_CREATE_DATE should be within reasonable range\", \"business_impact\": \"Invalid dates may indicate data corruption\", \"remediation\": \"Validate date ranges in source systems\"}}, {\"rule_id\": \"BUS_001\", \"category\": \"Important\", \"expectation_type\": \"expect_column_values_to_not_be_null\", \"kwargs\": {\"column\": \"STATUS\", \"mostly\": 0.95}, \"meta\": {\"description\": \"STATUS column should be mostly non-null\", \"business_impact\": \"Missing status affects operational reporting\", \"remediation\": \"Implement default status values\"}}, {\"rule_id\": \"DQL_001\", \"category\": \"Critical\", \"expectation_type\": \"expect_column_values_to_not_be_null\", \"kwargs\": {\"column\": \"LOGIC_TYPE\", \"mostly\": 0.98}, \"meta\": {\"description\": \"LOGIC_TYPE should be almost always populated for DQ_LOGIC table\", \"business_impact\": \"Missing logic types affect data quality categorization\", \"remediation\": \"Review data quality logic classification\"}}, {\"rule_id\": \"DQL_002\", \"category\": \"Important\", \"expectation_type\": \"expect_column_distinct_values_to_be_in_set\", \"kwargs\": {\"column\": \"ACTIVE_FLAG\", \"value_set\": [\"Y\", \"N\", \"1\", \"0\", \"TRUE\", \"FALSE\"]}, \"meta\": {\"description\": \"ACTIVE_FLAG should use standard boolean values\", \"business_impact\": \"Non-standard flags affect filtering logic\", \"remediation\": \"Standardize boolean value representation\"}}], \"table_specific_rules\": {\"DQ_LOGIC\": [{\"rule_id\": \"DQL_001\", \"category\": \"Critical\", \"expectation_type\": \"expect_column_values_to_not_be_null\", \"kwargs\": {\"column\": \"LOGIC_TYPE\", \"mostly\": 0.98}, \"meta\": {\"description\": \"LOGIC_TYPE should be almost always populated for DQ_LOGIC table\", \"business_impact\": \"Missing logic types affect data quality categorization\", \"remediation\": \"Review data quality logic classification\"}}, {\"rule_id\": \"DQL_002\", \"category\": \"Important\", \"expectation_type\": \"expect_column_distinct_values_to_be_in_set\", \"kwargs\": {\"column\": \"ACTIVE_FLAG\", \"value_set\": [\"Y\", \"N\", \"1\", \"0\", \"TRUE\", \"FALSE\"]}, \"meta\": {\"description\": \"ACTIVE_FLAG should use standard boolean values\", \"business_impact\": \"Non-standard flags affect filtering logic\", \"remediation\": \"Standardize boolean value representation\"}}], \"USER_PROFILES\": [{\"rule_id\": \"USR_001\", \"category\": \"Critical\", \"expectation_type\": \"expect_column_values_to_be_unique\", \"kwargs\": {\"column\": \"USER_ID\"}, \"meta\": {\"description\": \"USER_ID must be unique in USER_PROFILES\", \"business_impact\": \"Duplicate user IDs cause authentication issues\", \"remediation\": \"Implement unique constraints on user ID\"}}]}, \"error_message\": null, \"validation_results\": {\"valid_rules\": 12, \"invalid_rules\": 0, \"errors\": []}}"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "{\"status\": \"success\", \"rules_defined\": 12, \"rule_categories\": {\"Critical\": 6, \"Important\": 6, \"Optional\": 0}, \"custom_rules\": [{\"rule_id\": \"TBL_001\", \"category\": \"Critical\", \"expectation_type\": \"expect_table_row_count_to_be_between\", \"kwargs\": {\"min_value\": 1, \"max_value\": 1000000}, \"meta\": {\"description\": \"Table should have reasonable number of rows\", \"business_impact\": \"Empty tables or extremely large tables may indicate data pipeline issues\", \"remediation\": \"Check data pipeline and source systems\"}}, {\"rule_id\": \"TBL_002\", \"category\": \"Important\", \"expectation_type\": \"expect_table_column_count_to_be_between\", \"kwargs\": {\"min_value\": 1, \"max_value\": 100}, \"meta\": {\"description\": \"Table should have reasonable number of columns\", \"business_impact\": \"Schema changes may affect downstream systems\", \"remediation\": \"Verify table schema matches expectations\"}}, {\"rule_id\": \"HIE_001\", \"category\": \"Critical\", \"expectation_type\": \"expect_column_to_exist\", \"kwargs\": {\"column\": \"HIERARCHY_ID\"}, \"meta\": {\"description\": \"HIERARCHY_ID column must exist\", \"business_impact\": \"Missing hierarchy column breaks organizational structure\", \"remediation\": \"Verify table schema and data source\"}}, {\"rule_id\": \"HIE_002\", \"category\": \"Critical\", \"expectation_type\": \"expect_column_values_to_not_be_null\", \"kwargs\": {\"column\": \"HIERARCHY_ID\"}, \"meta\": {\"description\": \"HIERARCHY_ID should not be null\", \"business_impact\": \"Null hierarchy IDs break organizational reporting\", \"remediation\": \"Investigate source data quality and implement null checks\"}}, {\"rule_id\": \"HIE_003\", \"category\": \"Important\", \"expectation_type\": \"expect_column_values_to_be_unique\", \"kwargs\": {\"column\": \"HIERARCHY_ID\"}, \"meta\": {\"description\": \"HIERARCHY_ID should be unique\", \"business_impact\": \"Duplicate hierarchy IDs cause reporting inconsistencies\", \"remediation\": \"Implement unique constraints and deduplication logic\"}}, {\"rule_id\": \"HIE_004\", \"category\": \"Important\", \"expectation_type\": \"expect_column_values_to_match_regex\", \"kwargs\": {\"column\": \"HIERARCHY_ID\", \"regex\": \"^[A-Z0-9_-]+$\"}, \"meta\": {\"description\": \"HIERARCHY_ID should follow standard format\", \"business_impact\": \"Non-standard formats may cause integration issues\", \"remediation\": \"Standardize hierarchy ID format across systems\"}}, {\"rule_id\": \"DTE_001\", \"category\": \"Critical\", \"expectation_type\": \"expect_column_to_exist\", \"kwargs\": {\"column\": \"RECORD_CREATE_DATE\"}, \"meta\": {\"description\": \"RECORD_CREATE_DATE column must exist\", \"business_impact\": \"Missing audit dates affect compliance and traceability\", \"remediation\": \"Ensure audit columns are included in data pipeline\"}}, {\"rule_id\": \"DTE_002\", \"category\": \"Critical\", \"expectation_type\": \"expect_column_values_to_not_be_null\", \"kwargs\": {\"column\": \"RECORD_CREATE_DATE\"}, \"meta\": {\"description\": \"RECORD_CREATE_DATE should not be null\", \"business_impact\": \"Missing audit timestamps affect regulatory compliance\", \"remediation\": \"Implement default timestamp logic in data pipeline\"}}, {\"rule_id\": \"DTE_003\", \"category\": \"Important\", \"expectation_type\": \"expect_column_values_to_be_between\", \"kwargs\": {\"column\": \"RECORD_CREATE_DATE\", \"min_value\": \"2020-01-01\", \"max_value\": \"2030-12-31\"}, \"meta\": {\"description\": \"RECORD_CREATE_DATE should be within reasonable range\", \"business_impact\": \"Invalid dates may indicate data corruption\", \"remediation\": \"Validate date ranges in source systems\"}}, {\"rule_id\": \"BUS_001\", \"category\": \"Important\", \"expectation_type\": \"expect_column_values_to_not_be_null\", \"kwargs\": {\"column\": \"STATUS\", \"mostly\": 0.95}, \"meta\": {\"description\": \"STATUS column should be mostly non-null\", \"business_impact\": \"Missing status affects operational reporting\", \"remediation\": \"Implement default status values\"}}, {\"rule_id\": \"DQL_001\", \"category\": \"Critical\", \"expectation_type\": \"expect_column_values_to_not_be_null\", \"kwargs\": {\"column\": \"LOGIC_TYPE\", \"mostly\": 0.98}, \"meta\": {\"description\": \"LOGIC_TYPE should be almost always populated for DQ_LOGIC table\", \"business_impact\": \"Missing logic types affect data quality categorization\", \"remediation\": \"Review data quality logic classification\"}}, {\"rule_id\": \"DQL_002\", \"category\": \"Important\", \"expectation_type\": \"expect_column_distinct_values_to_be_in_set\", \"kwargs\": {\"column\": \"ACTIVE_FLAG\", \"value_set\": [\"Y\", \"N\", \"1\", \"0\", \"TRUE\", \"FALSE\"]}, \"meta\": {\"description\": \"ACTIVE_FLAG should use standard boolean values\", \"business_impact\": \"Non-standard flags affect filtering logic\", \"remediation\": \"Standardize boolean value representation\"}}], \"table_specific_rules\": {\"DQ_LOGIC\": [{\"rule_id\": \"DQL_001\", \"category\": \"Critical\", \"expectation_type\": \"expect_column_values_to_not_be_null\", \"kwargs\": {\"column\": \"LOGIC_TYPE\", \"mostly\": 0.98}, \"meta\": {\"description\": \"LOGIC_TYPE should be almost always populated for DQ_LOGIC table\", \"business_impact\": \"Missing logic types affect data quality categorization\", \"remediation\": \"Review data quality logic classification\"}}, {\"rule_id\": \"DQL_002\", \"category\": \"Important\", \"expectation_type\": \"expect_column_distinct_values_to_be_in_set\", \"kwargs\": {\"column\": \"ACTIVE_FLAG\", \"value_set\": [\"Y\", \"N\", \"1\", \"0\", \"TRUE\", \"FALSE\"]}, \"meta\": {\"description\": \"ACTIVE_FLAG should use standard boolean values\", \"business_impact\": \"Non-standard flags affect filtering logic\", \"remediation\": \"Standardize boolean value representation\"}}], \"USER_PROFILES\": [{\"rule_id\": \"USR_001\", \"category\": \"Critical\", \"expectation_type\": \"expect_column_values_to_be_unique\", \"kwargs\": {\"column\": \"USER_ID\"}, \"meta\": {\"description\": \"USER_ID must be unique in USER_PROFILES\", \"business_impact\": \"Duplicate user IDs cause authentication issues\", \"remediation\": \"Implement unique constraints on user ID\"}}]}, \"error_message\": null, \"validation_results\": {\"valid_rules\": 12, \"invalid_rules\": 0, \"errors\": []}}",
       "metadata": {},
       "type": "exit"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple JSON cleanup and return  \n",
    "def clean_for_json(obj):\n",
    "    \"\"\"Convert non-serializable types to JSON-compatible types\"\"\"\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_for_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_for_json(v) for v in obj]\n",
    "    elif isinstance(obj, (datetime.date, datetime.datetime, np.datetime64)):\n",
    "        return str(obj)\n",
    "    elif isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.bool_, bool)):\n",
    "        return bool(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Clean and return results\n",
    "step4_5_results = clean_for_json(step4_5_results)\n",
    "dbutils.notebook.exit(json.dumps(step4_5_results))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "step4_5_rules_definition",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}