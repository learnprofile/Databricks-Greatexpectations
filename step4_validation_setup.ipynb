{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4549e767-e1fe-443c-8d26-2f9b18ee1a68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 4: Data Quality Validation Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b05ca220-6819-4589-9c7c-85f4be65c621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python environment: /local_disk0/.ephemeral_nfs/envs/pythonEnv-f38b53fb-ed20-4ae5-94f7-129d43bc14bf/bin/python\nSUCCESS: Great Expectations 1.5.6 is available!\nModule location: /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/great_expectations/__init__.py\nReady to proceed with Step 4 validation setup\nDatabricks environment confirmed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "print(f\"Python environment: {sys.executable}\")\n",
    "\n",
    "# Step 1: Try to import Great Expectations\n",
    "try:\n",
    "    import great_expectations as gx\n",
    "    print(f\"SUCCESS: Great Expectations {gx.__version__} is available!\")\n",
    "    print(f\"Module location: {gx.__file__}\")\n",
    "    print(f\"Ready to proceed with Step 4 validation setup\")\n",
    "    \n",
    "    # Verify Databricks environment\n",
    "    try:\n",
    "        dbutils.fs.ls('/')\n",
    "        print(\"Databricks environment confirmed\")\n",
    "    except NameError:\n",
    "        print(\"Warning: dbutils not available\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Great Expectations not found - Auto-installing now...\")\n",
    "    \n",
    "    # Check if we're in Databricks\n",
    "    try:\n",
    "        dbutils.fs.ls('/')\n",
    "        print(\"Databricks environment confirmed\")\n",
    "        \n",
    "        print(\"AUTO-INSTALLING WITH DATABRICKS %pip...\")\n",
    "        print(\"Installing: great-expectations[sql,azure,databricks]...\")\n",
    "        \n",
    "        # Auto-install using %pip magic\n",
    "        get_ipython().run_line_magic('pip', 'install great-expectations[sql,azure,databricks]')\n",
    "        \n",
    "        print(\"Installation completed!\")\n",
    "        print(\"AUTO-RESTARTING PYTHON ENVIRONMENT...\")\n",
    "        \n",
    "  \n",
    "        dbutils.library.restartPython()\n",
    "        \n",
    "    except NameError:\n",
    "        print(\"Not in Databricks - manual installation required\")\n",
    "        print(\"MANUAL INSTALLATION REQUIRED:\")\n",
    "        print(\"pip install great-expectations[sql]\")\n",
    "        raise ImportError(\"Manual installation required - not in Databricks environment\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Auto-installation failed: {e}\")\n",
    "        print(\"FALLBACK - RUN THESE COMMANDS MANUALLY:\")\n",
    "        print(\"1. %pip install great-expectations[sql,azure,databricks]\")\n",
    "        print(\"2. dbutils.library.restartPython()\")\n",
    "        print(\"3. Re-run this cell\")\n",
    "        raise ImportError(\"Auto-installation failed - use manual commands above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c44524-01e2-4b0f-a5d6-edf1cb80f33c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File-based context recreated\nContext type: FileDataContext\nBasic expectation suite created: dq_logic_validation_suite\nValidation framework ready for expectation creation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import great_expectations as gx\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "step4_results = {\n",
    "    \"status\": \"running\",\n",
    "    \"context_recreated\": False,\n",
    "    \"datasource_created\": False,\n",
    "    \"expectation_suite_created\": False,\n",
    "    \"validator_created\": False,\n",
    "    \"validation_ready\": False,\n",
    "    \"suite_name\": \"dq_logic_validation_suite\",\n",
    "    \"datasource_name\": \"sql_server_datasource\",\n",
    "    \"asset_name\": \"dq_logic_table\",\n",
    "    \"error_message\": None\n",
    "}\n",
    "\n",
    "try:\n",
    "\n",
    "    \n",
    "    context = None\n",
    "    \n",
    "\n",
    "    try:\n",
    "        context = gx.get_context(project_root_dir=\"/dbfs/FileStore/great_expectations\")\n",
    "        print(\"File-based context recreated\")\n",
    "        step4_results[\"context_type\"] = \"FileDataContext\"\n",
    "    except:\n",
    "        try:\n",
    "            context = gx.get_context(mode=\"ephemeral\")\n",
    "            print(\"Ephemeral context created\")\n",
    "            step4_results[\"context_type\"] = \"EphemeralDataContext\"\n",
    "        except Exception as e:\n",
    "            print(f\"Context creation failed: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    step4_results[\"context_recreated\"] = True\n",
    "    print(f\"Context type: {type(context).__name__}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error recreating context: {e}\")\n",
    "    step4_results[\"status\"] = \"error\"\n",
    "    step4_results[\"error_message\"] = f\"Context recreation failed: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "if step4_results[\"context_recreated\"]:\n",
    "\n",
    "    \n",
    "    try:\n",
    "\n",
    "        suite_name = step4_results[\"suite_name\"]\n",
    "        \n",
    "\n",
    "        try:\n",
    "            suite = context.suites.add(gx.ExpectationSuite(name=suite_name))\n",
    "            print(f\"Expectation suite created: {suite_name}\")\n",
    "            step4_results[\"expectation_suite_created\"] = True\n",
    "        except Exception as e:\n",
    "\n",
    "            suite = gx.ExpectationSuite(name=suite_name)\n",
    "            print(f\"Basic expectation suite created: {suite_name}\")\n",
    "            step4_results[\"expectation_suite_created\"] = True\n",
    "        \n",
    "\n",
    "        step4_results[\"validation_framework_ready\"] = True\n",
    "        step4_results[\"status\"] = \"success\"\n",
    "        \n",
    "        print(f\"Validation framework ready for expectation creation\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Validation setup failed: {e}\")\n",
    "        step4_results[\"error_message\"] = f\"Validation setup failed: {e}\"\n",
    "        step4_results[\"status\"] = \"error\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db2babc2-9d67-41f3-bd84-ef9fecd05e03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_for_json(obj):\n",
    "    \"\"\"Convert non-serializable types to JSON-compatible types\"\"\"\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_for_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_for_json(v) for v in obj]\n",
    "    elif isinstance(obj, (datetime.date, datetime.datetime, np.datetime64)):\n",
    "        return str(obj)\n",
    "    elif isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.bool_, bool)):\n",
    "        return bool(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Clean and return results\n",
    "step4_results = clean_for_json(step4_results)\n",
    "dbutils.notebook.exit(json.dumps(step4_results))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "step4_validation_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}