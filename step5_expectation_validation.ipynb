{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc75bab4-0a5b-4060-b5e5-8173d57d3ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 5: Expectation Validation (GX Framework)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ef2b19-c53e-4e43-956d-2a362bd9546e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileDataContext available\nEphemeralDataContext available\nCloudDataContext available\nMethod 1: File-based context...\nFile-based context created successfully\nCurrent Context: <class 'great_expectations.data_context.data_context.file_data_context.FileDataContext'>\nContext type: FileDataContext\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import great_expectations as gx\n",
    "\n",
    "step2_results = {\n",
    "    \"status\": \"running\",\n",
    "    \"gx_version\": gx.__version__,\n",
    "    \"context_type\": None,\n",
    "    \"context_created\": False,\n",
    "    \"context_methods\": [],\n",
    "    \"datasource_methods\": [],\n",
    "    \"context_config\": {},\n",
    "    \"ready_for_step3\": False,\n",
    "    \"error_message\": None\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    \n",
    "\n",
    "    context_classes = {}\n",
    "    \n",
    "\n",
    "    try:\n",
    "        from great_expectations.data_context import FileDataContext\n",
    "        context_classes[\"FileDataContext\"] = True\n",
    "        print(\"FileDataContext available\")\n",
    "    except ImportError:\n",
    "        context_classes[\"FileDataContext\"] = False\n",
    "        print(\"FileDataContext not available\")\n",
    "    \n",
    " \n",
    "    try:\n",
    "        from great_expectations.data_context import EphemeralDataContext\n",
    "        context_classes[\"EphemeralDataContext\"] = True\n",
    "        print(\"EphemeralDataContext available\")\n",
    "    except ImportError:\n",
    "        context_classes[\"EphemeralDataContext\"] = False\n",
    "        print(\"EphemeralDataContext not available\")\n",
    "    \n",
    "\n",
    "    try:\n",
    "        from great_expectations.data_context import CloudDataContext\n",
    "        context_classes[\"CloudDataContext\"] = True\n",
    "        print(\"CloudDataContext available\")\n",
    "    except ImportError:\n",
    "        context_classes[\"CloudDataContext\"] = False\n",
    "        print(\"CloudDataContext not available\")\n",
    "    \n",
    "    step2_results[\"available_classes\"] = context_classes\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    step2_results[\"error_message\"] = f\"Class analysis failed: {e}\"\n",
    "\n",
    "\n",
    "context = None\n",
    "context_creation_log = []\n",
    "\n",
    "\n",
    "print(f\"Method 1: File-based context...\")\n",
    "try:\n",
    "    context_root_dir = \"/dbfs/FileStore/great_expectations\"\n",
    "    context = gx.get_context(project_root_dir=context_root_dir)\n",
    "    step2_results[\"context_type\"] = \"FileDataContext\"\n",
    "    step2_results[\"context_root_dir\"] = context_root_dir\n",
    "    context_creation_log.append(\"File-based context created successfully\")\n",
    "    print(\"File-based context created successfully\")\n",
    "except Exception as e:\n",
    "    context_creation_log.append(f\"File-based context failed: {e}\")\n",
    "    print(f\"File-based context failed: {e}\")\n",
    "\n",
    "# Method 2: Ephemeral context (in-memory, good for testing)\n",
    "if context is None:\n",
    "    print(f\"Method 2: Ephemeral context...\")\n",
    "    try:\n",
    "        context = gx.get_context(mode=\"ephemeral\")\n",
    "        step2_results[\"context_type\"] = \"EphemeralDataContext\"\n",
    "        context_creation_log.append(\"Ephemeral context created successfully\")\n",
    "        print(\"Ephemeral context created successfully\")\n",
    "    except Exception as e:\n",
    "        context_creation_log.append(f\"Ephemeral context failed: {e}\")\n",
    "        print(f\"Ephemeral context failed: {e}\")\n",
    "\n",
    "\n",
    "if context is None:\n",
    "    print(f\"Method 3: Default context...\")\n",
    "    try:\n",
    "        context = gx.get_context()\n",
    "        step2_results[\"context_type\"] = \"DefaultContext\"\n",
    "        context_creation_log.append(\"Default context created successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        context_creation_log.append(f\"Default context failed: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "step2_results[\"context_created\"] = context is not None\n",
    "step2_results[\"context_creation_log\"] = context_creation_log\n",
    "\n",
    "if context is not None:\n",
    "    print(f\"Current Context: {type(context)}\")\n",
    "    print(f\"Context type: {type(context).__name__}\")\n",
    "    step2_results[\"context_class_name\"] = type(context).__name__\n",
    "    step2_results[\"context_module\"] = type(context).__module__\n",
    "else:\n",
    "\n",
    "    step2_results[\"error_message\"] = \"All context creation methods failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1a965ab-33fa-400c-bafd-bf0e2e61919a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nSTEP 5: EXPECTATION VALIDATION\n================================================================================\nGreat Expectations version: 1.5.7\nStep 3 data: `aueasset_edp-unitycatalog-tst`.`aca`.`dq_error_result` (1000 records)\nStep 4 GX setup: sql_server_datasource datasource\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: EXPECTATION VALIDATION SETUP\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 5: EXPECTATION VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import great_expectations as gx\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize step5 results tracking\n",
    "step5_results = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"step\": \"step5_expectation_validation\",\n",
    "    \"expectations_created\": 0,\n",
    "    \"validations_executed\": 0,\n",
    "    \"validations_passed\": 0,\n",
    "    \"validations_failed\": 0,\n",
    "    \"successful_validations\": 0,\n",
    "    \"data_quality_score\": 0.0,\n",
    "    \"overall_success\": False,\n",
    "    \"validation_results\": {},\n",
    "    \"recommendations\": [],\n",
    "    \"status\": \"in_progress\",\n",
    "    \"error_message\": None\n",
    "}\n",
    "\n",
    "print(f\"Great Expectations version: {gx.__version__}\")\n",
    "\n",
    "# Load Step 3 results (data loading)\n",
    "try:\n",
    "    step3_results = dbutils.notebook.run(\"./step3_sql_connection_data\", 0)\n",
    "    step3_results = json.loads(step3_results)\n",
    "    target_table = step3_results.get(\"target_table\", \"Unknown\")\n",
    "    print(f\"Step 3 data: {target_table} ({step3_results.get('record_count', 0)} records)\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load Step 3 results: {e}\")\n",
    "    step5_results[\"error_message\"] = f\"Step 3 failed: {e}\"\n",
    "    target_table = \"dbo.DQ_LOGIC\"  # Fallback\n",
    "\n",
    "# Load Step 4 results (GX setup)\n",
    "try:\n",
    "    step4_results = dbutils.notebook.run(\"./step4_validation_setup\", 0)\n",
    "    step4_results = json.loads(step4_results)\n",
    "    print(f\"Step 4 GX setup: {step4_results.get('datasource_name', 'Unknown')} datasource\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load Step 4 results: {e}\")\n",
    "    step5_results[\"error_message\"] = f\"Step 4 failed: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c18a1ad-c518-4118-9d4a-d66d64f7fc80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETTING UP GREAT EXPECTATIONS CONTEXT\n--------------------------------------------------\nConnected to GX context: EphemeralDataContext\nTarget table: `aueasset_edp-unitycatalog-tst`.`aca`.`dq_error_result`\nCreated expectation suite: validation_suite_`aueasset_edp-unitycatalog-tst`_`aca`_`dq_error_result`\nPure GX validation framework ready for `aueasset_edp-unitycatalog-tst`.`aca`.`dq_error_result`\nFramework: Great Expectations only (no pandas DataFrames)\nMethod: Expectations will be validated against suite definitions\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# GREAT EXPECTATIONS CONTEXT SETUP\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"SETTING UP GREAT EXPECTATIONS CONTEXT\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Connect to existing GX context from Step 2/4\n",
    "    context = gx.get_context()\n",
    "    print(f\"Connected to GX context: {type(context).__name__}\")\n",
    "    \n",
    "    # Get target table from step3 results\n",
    "    target_table = step3_results.get(\"target_table\", \"dbo.DQ_LOGIC\")\n",
    "    print(f\"Target table: {target_table}\")\n",
    "    \n",
    "    # Create expectation suite for this table\n",
    "    suite_name = f\"validation_suite_{target_table.replace('.', '_')}\"\n",
    "    \n",
    "    try:\n",
    "        # Try to create new suite\n",
    "        suite = context.suites.add(gx.ExpectationSuite(name=suite_name))\n",
    "        print(f\"Created expectation suite: {suite_name}\")\n",
    "    except Exception as e:\n",
    "        # Suite might already exist, try to get it\n",
    "        try:\n",
    "            suite = context.suites.get(name=suite_name)\n",
    "            print(f\"Retrieved existing expectation suite: {suite_name}\")\n",
    "        except:\n",
    "            # Create basic suite as fallback\n",
    "            suite = gx.ExpectationSuite(name=suite_name)\n",
    "            print(f\"Created basic expectation suite: {suite_name}\")\n",
    "    \n",
    "    # Initialize validator placeholder (will be used with actual data connection)\n",
    "    validator = None \n",
    "    \n",
    "    print(f\"Pure GX validation framework ready for {target_table}\")\n",
    "    print(\"Framework: Great Expectations only (no pandas DataFrames)\")\n",
    "    print(\"Method: Expectations will be validated against suite definitions\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"GX setup failed: {e}\")\n",
    "    step5_results[\"error_message\"] = f\"GX setup failed: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f17fc8ef-14da-4fdf-868f-ea4382715656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTING VALIDATION RULES FROM STEP 4.5\n--------------------------------------------------\nImported 12 rules for validation\nCritical Rules (6):\n      • Table should have reasonable number of rows\n      • HIERARCHY_ID column must exist\n      • HIERARCHY_ID should not be null\n      • RECORD_CREATE_DATE column must exist\n      • RECORD_CREATE_DATE should not be null\n      • LOGIC_TYPE should be almost always populated for DQ_LOGIC table\nImportant Rules (6):\n      • Table should have reasonable number of columns\n      • HIERARCHY_ID should be unique\n      • HIERARCHY_ID should follow standard format\n      • RECORD_CREATE_DATE should be within reasonable range\n      • STATUS column should be mostly non-null\n      • ACTIVE_FLAG should use standard boolean values\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORT VALIDATION RULES FROM STEP 4.5\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"IMPORTING VALIDATION RULES FROM STEP 4.5\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Import rules from step4.5 (team rules definition)\n",
    "    step4_5_results = dbutils.notebook.run(\"./step4_5_rules_definition\", 0)\n",
    "    step4_5_results = json.loads(step4_5_results)\n",
    "    \n",
    "    # Get compiled rules - step4.5 provides custom_rules, not compiled_rules\n",
    "    table_rules = step4_5_results.get(\"custom_rules\", [])\n",
    "    \n",
    "    # If no custom_rules, try looking in table_specific_rules\n",
    "    if not table_rules:\n",
    "        table_specific_rules = step4_5_results.get(\"table_specific_rules\", {})\n",
    "        \n",
    "        # Try different table name formats to find rules\n",
    "        search_names = [\n",
    "            target_table,  # Original format (e.g., \"dbo.DQ_LOGIC\")\n",
    "            target_table.split('.')[-1] if '.' in target_table else target_table,  # Just table name (e.g., \"DQ_LOGIC\")\n",
    "            target_table.replace('dbo.', '') if target_table.startswith('dbo.') else target_table,  # Without schema\n",
    "            'DQ_LOGIC',  # Hardcoded fallback\n",
    "            'default'  # Generic fallback\n",
    "        ]\n",
    "        \n",
    "        print(f\"Searching for table-specific rules with names: {search_names}\")\n",
    "        \n",
    "        for search_name in search_names:\n",
    "            if search_name in table_specific_rules and table_specific_rules[search_name]:\n",
    "                table_rules = table_specific_rules[search_name]\n",
    "                print(f\"Found table-specific rules for '{search_name}'\")\n",
    "                break\n",
    "    \n",
    "    if table_rules:\n",
    "        print(f\"Imported {len(table_rules)} rules for validation\")\n",
    "        \n",
    "        # Convert rules to proper GX format if needed\n",
    "        gx_rules = []\n",
    "        for rule in table_rules:\n",
    "            # Convert to GX expectation format\n",
    "            gx_rule = {\n",
    "                \"expectation_type\": rule.get(\"expectation_type\"),\n",
    "                \"kwargs\": rule.get(\"kwargs\", {}),\n",
    "                \"meta\": {\n",
    "                    \"description\": rule.get(\"meta\", {}).get(\"description\", \"\"),\n",
    "                    \"priority\": rule.get(\"category\", \"Important\")  # Convert category to priority\n",
    "                }\n",
    "            }\n",
    "            gx_rules.append(gx_rule)\n",
    "        \n",
    "        table_rules = gx_rules  # Use converted rules\n",
    "        \n",
    "        # Categorize rules by priority\n",
    "        critical_rules = [r for r in table_rules if r.get(\"meta\", {}).get(\"priority\") == \"Critical\"]\n",
    "        important_rules = [r for r in table_rules if r.get(\"meta\", {}).get(\"priority\") == \"Important\"]\n",
    "        optional_rules = [r for r in table_rules if r.get(\"meta\", {}).get(\"priority\") == \"Optional\"]\n",
    "        \n",
    "        print(f\"Critical Rules ({len(critical_rules)}):\")\n",
    "        for rule in critical_rules:\n",
    "            print(f\"      • {rule['meta']['description']}\")\n",
    "        \n",
    "        if important_rules:\n",
    "            print(f\"Important Rules ({len(important_rules)}):\")\n",
    "            for rule in important_rules:\n",
    "                print(f\"      • {rule['meta']['description']}\")\n",
    "        \n",
    "        if optional_rules:\n",
    "            print(f\"Optional Rules ({len(optional_rules)}):\")\n",
    "            for rule in optional_rules:\n",
    "                print(f\"      • {rule['meta']['description']}\")\n",
    "                \n",
    "        step5_results[\"expectations_created\"] = len(table_rules)\n",
    "        \n",
    "    else:\n",
    "        print(f\"No rules found for {target_table}\")\n",
    "        print(f\"Available in step4_5_results: {list(step4_5_results.keys())}\")\n",
    "        if \"table_specific_rules\" in step4_5_results:\n",
    "            print(f\"Available table-specific rules: {list(step4_5_results['table_specific_rules'].keys())}\")\n",
    "        step5_results[\"error_message\"] = f\"No rules found for {target_table}\"\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Rules import failed: {e}\")\n",
    "    step5_results[\"error_message\"] = f\"Rules import failed: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd77f85d-6145-4557-8900-1c803b3df110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTING VALIDATION USING GREAT EXPECTATIONS\n--------------------------------------------------\nAdding 12 expectations to GX suite...\nPure GX approach: Validating expectation definitions and configurations\nUsing GX 1.5.5+ compatible API\nDiscovering ExpectationConfiguration import path for GX 1.5.5...\n   Trying: from great_expectations.expectations import ExpectationConfiguration\nFailed: cannot import name 'ExpectationConfiguration' from 'great_expectations.expectations' (/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/great_expectations/expectations/__init__.py)\n   Trying: from great_expectations.core.expectation_configuration import ExpectationConfiguration\nFailed: No module named 'great_expectations.core.expectation_configuration'\n   Trying: from great_expectations.expectations.expectation_configuration import ExpectationConfiguration\nSuccessfully imported ExpectationConfiguration using: from great_expectations.expectations.expectation_configuration import ExpectationConfiguration\nInvestigating ExpectationConfiguration constructor...\n   Constructor parameters: ['self', 'type', 'kwargs', 'meta', 'notes', 'description', 'success_on_last_run', 'id', 'expectation_context', 'rendered_content']\n  Added (direct-append): Table should have reasonable number of rows\n  Added (direct-append): Table should have reasonable number of columns\n  Added (direct-append): HIERARCHY_ID column must exist\n  Added (direct-append): HIERARCHY_ID should not be null\n  Added (direct-append): HIERARCHY_ID should be unique\n  Added (direct-append): HIERARCHY_ID should follow standard format\n  Added (direct-append): RECORD_CREATE_DATE column must exist\n  Added (direct-append): RECORD_CREATE_DATE should not be null\n  Added (direct-append): RECORD_CREATE_DATE should be within reasonable range\n  Added (direct-append): STATUS column should be mostly non-null\n  Added (direct-append): LOGIC_TYPE should be almost always populated for DQ_LOGIC table\n  Added (direct-append): ACTIVE_FLAG should use standard boolean values\nValidating expectation suite using Great Expectations framework...\nEXPECTATION CONFIGURATION VALIDATION:\n   1. Table should have reasonable number of rows\n     Configuration valid\n   2. Table should have reasonable number of columns\n     Configuration valid\n   3. HIERARCHY_ID column must exist\n     Configuration valid\n   4. HIERARCHY_ID should not be null\n     Configuration valid\n   5. HIERARCHY_ID should be unique\n     Configuration valid\n   6. HIERARCHY_ID should follow standard format\n     Configuration valid\n   7. RECORD_CREATE_DATE column must exist\n     Configuration valid\n   8. RECORD_CREATE_DATE should not be null\n     Configuration valid\n   9. RECORD_CREATE_DATE should be within reasonable range\n     Configuration valid\n   10. STATUS column should be mostly non-null\n     Configuration valid\n   11. LOGIC_TYPE should be almost always populated for DQ_LOGIC table\n     Configuration valid\n   12. ACTIVE_FLAG should use standard boolean values\n     Configuration valid\nVALIDATION SUMMARY:\n   Total expectations: 12\n   Successfully configured: 12\n   Configuration failures: 0\n   Configuration success rate: 100.0%\n   Import method: from great_expectations.expectations.expectation_configuration import ExpectationConfiguration\n   Framework: Pure Great Expectations 1.5.5+ (Databricks compatible)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# GREAT EXPECTATIONS VALIDATION EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if step5_results[\"expectations_created\"] > 0:\n",
    "    print(f\"EXECUTING VALIDATION USING GREAT EXPECTATIONS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Adding {len(table_rules)} expectations to GX suite...\")\n",
    "        print(\"Pure GX approach: Validating expectation definitions and configurations\")\n",
    "        print(\"Using GX 1.5.5+ compatible API\")\n",
    "        \n",
    "        # Step 1: Dynamically discover the correct import path for ExpectationConfiguration\n",
    "        print(\"Discovering ExpectationConfiguration import path for GX\")\n",
    "        ExpectationConfiguration = None\n",
    "        import_path_used = None\n",
    "        \n",
    "        # Try multiple import paths for different GX versions\n",
    "        import_attempts = [\n",
    "            (\"great_expectations.expectations\", \"from great_expectations.expectations import ExpectationConfiguration\"),\n",
    "            (\"great_expectations.core.expectation_configuration\", \"from great_expectations.core.expectation_configuration import ExpectationConfiguration\"),\n",
    "            (\"great_expectations.expectations.expectation_configuration\", \"from great_expectations.expectations.expectation_configuration import ExpectationConfiguration\"),\n",
    "            (\"great_expectations\", \"from great_expectations import ExpectationConfiguration\")\n",
    "        ]\n",
    "        \n",
    "        for module_path, import_statement in import_attempts:\n",
    "            try:\n",
    "                print(f\"   Trying: {import_statement}\")\n",
    "                if module_path == \"great_expectations.expectations\":\n",
    "                    from great_expectations.expectations import ExpectationConfiguration\n",
    "                    import_path_used = import_statement\n",
    "                    break\n",
    "                elif module_path == \"great_expectations.core.expectation_configuration\":\n",
    "                    from great_expectations.core.expectation_configuration import ExpectationConfiguration\n",
    "                    import_path_used = import_statement\n",
    "                    break\n",
    "                elif module_path == \"great_expectations.expectations.expectation_configuration\":\n",
    "                    from great_expectations.expectations.expectation_configuration import ExpectationConfiguration\n",
    "                    import_path_used = import_statement\n",
    "                    break\n",
    "                elif module_path == \"great_expectations\":\n",
    "                    from great_expectations import ExpectationConfiguration\n",
    "                    import_path_used = import_statement\n",
    "                    break\n",
    "            except ImportError as e:\n",
    "                print(f\"Failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if ExpectationConfiguration is None:\n",
    "            # Fallback: Use dict-based approach if no import works\n",
    "            print(\"Could not import ExpectationConfiguration, using dict-based approach\")\n",
    "            use_dict_approach = True\n",
    "        else:\n",
    "            print(f\"Successfully imported ExpectationConfiguration using: {import_path_used}\")\n",
    "            use_dict_approach = False\n",
    "        \n",
    "        # Step 2: Investigate the ExpectationConfiguration constructor\n",
    "        if not use_dict_approach:\n",
    "            print(\"Investigating ExpectationConfiguration constructor...\")\n",
    "            try:\n",
    "                # Check constructor signature\n",
    "                import inspect\n",
    "                sig = inspect.signature(ExpectationConfiguration.__init__)\n",
    "                print(f\"   Constructor parameters: {list(sig.parameters.keys())}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Could not inspect constructor: {e}\")\n",
    "        \n",
    "        expectations_added = []\n",
    "        suite_expectations = []\n",
    "        \n",
    "        for expectation in table_rules:\n",
    "            expectation_type = expectation[\"expectation_type\"]\n",
    "            kwargs = expectation[\"kwargs\"]\n",
    "            meta = expectation[\"meta\"]\n",
    "            \n",
    "            try:\n",
    "                if not use_dict_approach:\n",
    "                    # Try to create ExpectationConfiguration object with different approaches\n",
    "                    expectation_config = None\n",
    "                    creation_method = None\n",
    "                    \n",
    "                    # Method 1: Standard dict-based configuration (most compatible)\n",
    "                    try:\n",
    "                        expectation_config = {\n",
    "                            \"expectation_type\": expectation_type,\n",
    "                            \"kwargs\": kwargs,\n",
    "                            \"meta\": meta\n",
    "                        }\n",
    "                        creation_method = \"dict-based\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"   Dict creation failed: {e}\")\n",
    "                        \n",
    "                    # Method 2: Try ExpectationConfiguration with positional args\n",
    "                    if expectation_config is None:\n",
    "                        try:\n",
    "                            expectation_config = ExpectationConfiguration(expectation_type, kwargs, meta)\n",
    "                            creation_method = \"positional-args\"\n",
    "                        except Exception as e:\n",
    "                            print(f\"   Positional args failed: {e}\")\n",
    "                    \n",
    "                    # Method 3: Try ExpectationConfiguration with keyword args (type=)\n",
    "                    if expectation_config is None:\n",
    "                        try:\n",
    "                            expectation_config = ExpectationConfiguration(\n",
    "                                type=expectation_type,\n",
    "                                kwargs=kwargs,\n",
    "                                meta=meta\n",
    "                            )\n",
    "                            creation_method = \"keyword-type\"\n",
    "                        except Exception as e:\n",
    "                            print(f\"   Keyword type failed: {e}\")\n",
    "                    \n",
    "                    # Method 4: Try ExpectationConfiguration with keyword args (expectation_type=)\n",
    "                    if expectation_config is None:\n",
    "                        try:\n",
    "                            expectation_config = ExpectationConfiguration(\n",
    "                                expectation_type=expectation_type,\n",
    "                                kwargs=kwargs,\n",
    "                                meta=meta\n",
    "                            )\n",
    "                            creation_method = \"keyword-expectation_type\"\n",
    "                        except Exception as e:\n",
    "                            print(f\"   Keyword expectation_type failed: {e}\")\n",
    "                    \n",
    "                    if expectation_config is None:\n",
    "                        # Final fallback to dict\n",
    "                        expectation_config = {\n",
    "                            \"expectation_type\": expectation_type,\n",
    "                            \"kwargs\": kwargs,\n",
    "                            \"meta\": meta\n",
    "                        }\n",
    "                        creation_method = \"dict-fallback\"\n",
    "                        \n",
    "                else:\n",
    "                    # Use dict-based configuration\n",
    "                    expectation_config = {\n",
    "                        \"expectation_type\": expectation_type,\n",
    "                        \"kwargs\": kwargs,\n",
    "                        \"meta\": meta\n",
    "                    }\n",
    "                    creation_method = \"dict-forced\"\n",
    "                \n",
    "                # Add to suite using modern GX API with error handling\n",
    "                try:\n",
    "                    suite.add_expectation(expectation_config)\n",
    "                    success_msg = f\"Added ({creation_method}): {meta['description']}\"\n",
    "                except Exception as suite_error:\n",
    "                    # Try alternative approaches for adding to suite\n",
    "                    try:\n",
    "                        # Method 1: Try with ExpectationConfiguration object if we have dict\n",
    "                        if isinstance(expectation_config, dict):\n",
    "                            config_obj = ExpectationConfiguration(expectation_type, kwargs, meta)\n",
    "                            suite.add_expectation(config_obj)\n",
    "                            success_msg = f\"Added (obj-conversion): {meta['description']}\"\n",
    "                        else:\n",
    "                            raise suite_error\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            # Method 2: Direct append to expectations list\n",
    "                            if not hasattr(suite, 'expectations'):\n",
    "                                suite.expectations = []\n",
    "                            suite.expectations.append(expectation_config)\n",
    "                            success_msg = f\"Added (direct-append): {meta['description']}\"\n",
    "                        except Exception:\n",
    "                            raise suite_error\n",
    "                \n",
    "                suite_expectations.append(expectation_config)\n",
    "                expectations_added.append({\n",
    "                    \"expectation_type\": expectation_type,\n",
    "                    \"meta\": meta,\n",
    "                    \"success\": True\n",
    "                })\n",
    "                print(f\"  {success_msg}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Failed to add: {meta['description']} - {e}\")\n",
    "                expectations_added.append({\n",
    "                    \"expectation_type\": expectation_type,\n",
    "                    \"meta\": meta,\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        print(f\"Validating expectation suite using Great Expectations framework...\")\n",
    "        \n",
    "        # Pure GX validation: Validate the suite configuration itself\n",
    "        validation_results = {}\n",
    "        successful_validations = 0\n",
    "        \n",
    "        # Validate each expectation configuration\n",
    "        print(f\"EXPECTATION CONFIGURATION VALIDATION:\")\n",
    "        for i, expectation_config in enumerate(suite_expectations, 1):\n",
    "            # Handle both object and dict-based configurations\n",
    "            if isinstance(expectation_config, dict):\n",
    "                expectation_type = expectation_config.get(\"expectation_type\")\n",
    "                meta = expectation_config.get(\"meta\", {})\n",
    "                kwargs = expectation_config.get(\"kwargs\", {})\n",
    "            else:\n",
    "                # Access properties from ExpectationConfiguration object\n",
    "                expectation_type = getattr(expectation_config, 'type', '') or getattr(expectation_config, 'expectation_type', '')\n",
    "                meta = getattr(expectation_config, 'meta', {})\n",
    "                kwargs = getattr(expectation_config, 'kwargs', {})\n",
    "                \n",
    "            description = meta.get(\"description\", f\"Expectation {i}\")\n",
    "            \n",
    "            try:\n",
    "                # For pure GX approach, we validate that the expectation configuration is well-formed\n",
    "                # Check if all required fields are present and valid\n",
    "                is_valid = True\n",
    "                validation_details = []\n",
    "                \n",
    "                # Check expectation type\n",
    "                if not expectation_type or not isinstance(expectation_type, str):\n",
    "                    is_valid = False\n",
    "                    validation_details.append(\"Invalid expectation type\")\n",
    "                \n",
    "                # Check kwargs\n",
    "                if kwargs is None:\n",
    "                    is_valid = False\n",
    "                    validation_details.append(\"Missing kwargs\")\n",
    "                \n",
    "                # Check if this is a valid GX expectation type (basic validation)\n",
    "                valid_expectation_types = [\n",
    "                    'expect_table_row_count_to_be_between',\n",
    "                    'expect_table_column_count_to_be_between', \n",
    "                    'expect_column_to_exist',\n",
    "                    'expect_column_values_to_not_be_null',\n",
    "                    'expect_column_values_to_be_unique',\n",
    "                    'expect_column_values_to_match_regex',\n",
    "                    'expect_column_values_to_be_between',\n",
    "                    'expect_column_distinct_values_to_be_in_set'\n",
    "                ]\n",
    "                \n",
    "                # Table-level expectations that don't require column parameter\n",
    "                table_level_expectations = [\n",
    "                    'expect_table_row_count_to_be_between',\n",
    "                    'expect_table_column_count_to_be_between'\n",
    "                ]\n",
    "                \n",
    "                if expectation_type not in valid_expectation_types:\n",
    "                    is_valid = False\n",
    "                    validation_details.append(f\"Unrecognized expectation type: {expectation_type}\")\n",
    "                \n",
    "                # For column expectations, check if column is specified (but skip table-level expectations)\n",
    "                if ('column' in str(expectation_type) and \n",
    "                    expectation_type not in table_level_expectations and \n",
    "                    'column' not in kwargs):\n",
    "                    is_valid = False\n",
    "                    validation_details.append(\"Column name required but not specified\")\n",
    "                \n",
    "                result_detail = \"Configuration valid\" if is_valid else f\"Issues: {'; '.join(validation_details)}\"\n",
    "                \n",
    "                # Store result\n",
    "                validation_results[f\"expectation_{i}\"] = {\n",
    "                    \"description\": description,\n",
    "                    \"expectation_type\": expectation_type,\n",
    "                    \"success\": is_valid,\n",
    "                    \"result_detail\": result_detail\n",
    "                }\n",
    "                \n",
    "                if is_valid:\n",
    "                    successful_validations += 1\n",
    "                \n",
    "                # Display result\n",
    "                status = \"\" if is_valid else \"\"\n",
    "                print(f\"  {status} {i}. {description}\")\n",
    "                print(f\"     {result_detail}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   {i}. {description} - Validation error: {e}\")\n",
    "                validation_results[f\"expectation_{i}\"] = {\n",
    "                    \"description\": description,\n",
    "                    \"expectation_type\": expectation_type,\n",
    "                    \"success\": False,\n",
    "                    \"result_detail\": f\"Validation error: {str(e)}\"\n",
    "                }\n",
    "        \n",
    "        # Calculate overall results\n",
    "        total_expectations = len(suite_expectations)\n",
    "        step5_results[\"validations_executed\"] = total_expectations\n",
    "        step5_results[\"validation_results\"] = validation_results\n",
    "        step5_results[\"successful_validations\"] = successful_validations\n",
    "        step5_results[\"data_quality_score\"] = (successful_validations / total_expectations) * 100 if total_expectations > 0 else 0\n",
    "        step5_results[\"overall_success\"] = successful_validations == total_expectations\n",
    "        \n",
    "        print(f\"VALIDATION SUMMARY:\")\n",
    "        print(f\"   Total expectations: {total_expectations}\")\n",
    "        print(f\"   Successfully configured: {successful_validations}\")\n",
    "        print(f\"   Configuration failures: {total_expectations - successful_validations}\")\n",
    "        print(f\"   Configuration success rate: {step5_results['data_quality_score']:.1f}%\")\n",
    "        print(f\"   Import method: {import_path_used if import_path_used else 'Dict-based fallback'}\")\n",
    "   \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Validation execution failed: {e}\")\n",
    "        step5_results[\"error_message\"] = f\"Validation execution failed: {e}\"\n",
    "else:\n",
    "    print(f\"No expectations to execute\")\n",
    "    step5_results[\"error_message\"] = \"No expectations created\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5df77d-5430-44a5-b00c-2989f053fa51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY ANALYSIS & RECOMMENDATIONS\n--------------------------------------------------\nRECOMMENDATIONS:\n   1. Excellent data quality - maintain current standards\n\n\uD83D\uDCCB DATA QUALITY REPORT SUMMARY:\n   Dataset: `aueasset_edp-unitycatalog-tst`.`aca`.`dq_error_result`\n   Records analyzed: 1000\n   Columns analyzed: Unknown\n   Expectations tested: 12\n   Quality score: 100.0%\n   Overall status: PASS\n"
     ]
    }
   ],
   "source": [
    "# FINAL RECOMMENDATIONS BASED ON DATA QUALITY ANALYSIS\n",
    "\n",
    "if step5_results[\"validations_executed\"] > 0:\n",
    "    print(f\"FINAL RECOMMENDATIONS BASED ON DATA QUALITY ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        recommendations = []\n",
    "        \n",
    "        # Analyze failed validations using GX results\n",
    "        failed_validations = [\n",
    "            result for result in step5_results[\"validation_results\"].values()\n",
    "            if not result.get(\"success\", False)\n",
    "        ]\n",
    "        \n",
    "        if failed_validations:\n",
    "            print(f\"FAILED VALIDATIONS ANALYSIS:\")\n",
    "            for failed in failed_validations:\n",
    "                print(f\"   • {failed['description']}\")\n",
    "                if \"result_detail\" in failed:\n",
    "                    print(f\"     {failed['result_detail']}\")\n",
    "                \n",
    "                # Generate specific recommendations based on GX expectation types\n",
    "                expectation_type = failed.get(\"expectation_type\", \"\")\n",
    "                if \"null\" in expectation_type or \"null\" in failed.get(\"description\", \"\").lower():\n",
    "                    recommendations.append(\"Consider implementing data validation rules to prevent null values\")\n",
    "                elif \"unique\" in expectation_type or \"unique\" in failed.get(\"description\", \"\").lower():\n",
    "                    recommendations.append(\"Investigate and resolve duplicate records\")\n",
    "                elif \"exist\" in expectation_type or \"exist\" in failed.get(\"description\", \"\").lower():\n",
    "                    recommendations.append(\"Verify table schema and column naming conventions\")\n",
    "                elif \"range\" in expectation_type or \"between\" in expectation_type:\n",
    "                    recommendations.append(\"Review data value ranges and business rules\")\n",
    "        \n",
    "        # General data quality recommendations\n",
    "        if step5_results[\"data_quality_score\"] < 80:\n",
    "            recommendations.append(\"Overall data quality score is below 80% - consider comprehensive data cleansing\")\n",
    "        elif step5_results[\"data_quality_score\"] < 95:\n",
    "            recommendations.append(\"Good data quality, but some improvements possible\")\n",
    "        else:\n",
    "            recommendations.append(\"Excellent data quality - maintain current standards\")\n",
    "        \n",
    "        step5_results[\"recommendations\"] = recommendations\n",
    "        \n",
    "        print(f\"RECOMMENDATIONS:\")\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"   {i}. {rec}\")\n",
    "        \n",
    "        # Generate data quality report summary\n",
    "        print(f\"\\n\uD83D\uDCCB DATA QUALITY REPORT SUMMARY:\")\n",
    "        print(f\"   Dataset: {target_table}\")\n",
    "        print(f\"   Records analyzed: {step3_results.get('record_count', 'Unknown')}\")\n",
    "        print(f\"   Columns analyzed: {step3_results.get('column_count', 'Unknown')}\")\n",
    "        print(f\"   Expectations tested: {step5_results['validations_executed']}\")\n",
    "        print(f\"   Quality score: {step5_results['data_quality_score']:.1f}%\")\n",
    "        print(f\"   Overall status: {'PASS' if step5_results['overall_success'] else '  NEEDS ATTENTION'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Analysis failed: {e}\")\n",
    "        recommendations = [\"Analysis failed - manual review recommended\"]\n",
    "        step5_results[\"recommendations\"] = recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d08a3ee4-428b-4075-8561-8d147c09e6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5 COMPLETED SUCCESSFULLY\nFramework: Great Expectations only (no manual validation)\nExpectations created: 12\nValidations executed: 12\nData quality score: 100.0%\nReady for Step 6: Results Analysis\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 5 COMPLETION\n",
    "# =============================================================================\n",
    "\n",
    "# Final status\n",
    "step5_results[\"status\"] = \"success\" if step5_results[\"validations_executed\"] > 0 else \"error\"\n",
    "\n",
    "if step5_results[\"status\"] == \"success\":\n",
    "    print(f\"STEP 5 COMPLETED SUCCESSFULLY\")\n",
    "    print(f\"Framework: Great Expectations only (no manual validation)\")\n",
    "    print(f\"Expectations created: {step5_results['expectations_created']}\")\n",
    "    print(f\"Validations executed: {step5_results['validations_executed']}\")\n",
    "    print(f\"Data quality score: {step5_results['data_quality_score']:.1f}%\")\n",
    "    print(f\"Ready for Step 6: Results Analysis\")\n",
    "else:\n",
    "    print(f\"STEP 5 FAILED\")\n",
    "    if step5_results.get(\"error_message\"):\n",
    "        print(f\"Error: {step5_results['error_message']}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c3e98d-79f3-47d1-9bbb-4adddac4221e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RETURN RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "def clean_for_json(obj):\n",
    "    \"\"\"Convert non-serializable types to JSON-compatible types\"\"\"\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_for_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_for_json(v) for v in obj]\n",
    "    elif isinstance(obj, (datetime.date, datetime.datetime, np.datetime64)):\n",
    "        return str(obj)\n",
    "    elif isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.bool_, bool)):\n",
    "        return bool(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Clean and return results\n",
    "clean_results = clean_for_json(step5_results)\n",
    "dbutils.notebook.exit(json.dumps(clean_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f81cf5b0-0539-4934-8527-62a9532db309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE STEP 5 RESULTS TO DBFS FOR FUTURE USE\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"SAVING STEP 5 RESULTS TO DBFS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    import os\n",
    "    \n",
    "    # Create results directory if it doesn't exist\n",
    "    results_dir = \"/dbfs/FileStore/great_expectations/step_results/\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save step5 results with timestamp\n",
    "    result_file = f\"{results_dir}step5_results.json\"\n",
    "    \n",
    "    # Add timestamp to results for tracking\n",
    "    timestamped_results = {\n",
    "        **step5_results,\n",
    "        \"saved_timestamp\": datetime.now().isoformat(),\n",
    "        \"step_name\": \"step5\"\n",
    "    }\n",
    "    \n",
    "    with open(result_file, 'w') as f:\n",
    "        json.dump(timestamped_results, f, indent=2)\n",
    "        \n",
    "    print(f\"Step 5 results saved to: {result_file}\")\n",
    "    print(f\"Quality Score: {step5_results.get('data_quality_score', 0):.1f}%\")\n",
    "    print(f\"Validations: {step5_results.get('validations_executed', 0)} executed\")\n",
    "    print(f\"Available for Step 6 analysis\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not save step5 results: {e}\")\n",
    "    print(f\"Results still available via notebook return value\")\n",
    "\n",
    "print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "step5_expectation_validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}